[{"authors":null,"categories":["R","Tutorial"],"content":" lavaan: A Package for SEM Syntax for Modelling Model Fitting   lavaan: A Package for SEM In my pervious post I made a short introduction to Structural Equation Modeling (SEM). But now it’s about using R for SEM. However, to use R for SEM we need the package lavaan. For the visualization of the models we also need the package semPlot. As you probably know already the installation of the packages is very simple:\ninstall.packages(\u0026quot;lavaan\u0026quot;) install.packages(\u0026quot;semPlot\u0026quot;)  Syntax for Modelling The modely syntax of lavaan is used for specifying a SEM. There are three operators for model specification:\n =~ is used to specify latent variables/factors ~~ is used to specify (residual) (co)variances ~ is used to specify rgeression  So kann mit diesen Operatoren folgendes Beispielmodell spezfiziert werden:\nexample \u0026lt;- \u0026quot; # measurement model F1 =~ 1*V1 + V2 + V3 F2 =~ 1*V4 + V5 + V6 # (residual) (co)variances V1 ~~ V1 # variance V1 ~~ V3 # residual correlation # regressions F1 ~ F2\u0026quot;  Model Fitting Now, let’s specify the first model. I will use an own dataset from a previous work. It’s also possible to use the built-in datasets HolzingerSwineford1939 or PolitcialDemocracy from lavaan to become familiar with the package. Moreover, the functions cfa() and sem() are currently similar but they may differ in the future.\nlibrary(lavaan) model \u0026lt;- \u0026quot; # measurement model F1 =~ 1*X1 + X2 + X3 + X4 + X5 + X6 F2 =~ 1*Y1 + Y2 + Y3 + Y4 # regression F1 ~ F2\u0026quot; # let\u0026#39;s fit the model fit \u0026lt;- cfa(model, data = data) # summary of the fit with standardized values and fit measurements summary(fit, standardized = T, fit.measures = T) ## lavaan 0.6-3 ended normally after 39 iterations ## ## Optimization method NLMINB ## Number of free parameters 21 ## ## Number of observations 1914 ## ## Estimator ML ## Model Fit Test Statistic 218.644 ## Degrees of freedom 34 ## P-value (Chi-square) 0.000 ## ## Model test baseline model: ## ## Minimum Function Test Statistic 4983.424 ## Degrees of freedom 45 ## P-value 0.000 ## ## User model versus baseline model: ## ## Comparative Fit Index (CFI) 0.963 ## Tucker-Lewis Index (TLI) 0.951 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -39038.962 ## Loglikelihood unrestricted model (H1) -38929.641 ## ## Number of free parameters 21 ## Akaike (AIC) 78119.925 ## Bayesian (BIC) 78236.621 ## Sample-size adjusted Bayesian (BIC) 78169.903 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.053 ## 90 Percent Confidence Interval 0.047 0.060 ## P-value RMSEA \u0026lt;= 0.05 0.203 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.036 ## ## Parameter Estimates: ## ## Information Expected ## Information saturated (h1) model Structured ## Standard Errors Standard ## ## Latent Variables: ## Estimate Std.Err z-value P(\u0026gt;|z|) Std.lv Std.all ## F1 =~ ## X1 1.000 1.197 0.652 ## X2 0.839 0.043 19.375 0.000 1.004 0.541 ## X3 0.857 0.037 23.071 0.000 1.026 0.679 ## X4 1.035 0.046 22.288 0.000 1.239 0.646 ## X5 0.884 0.041 21.644 0.000 1.058 0.621 ## X6 0.916 0.044 20.634 0.000 1.096 0.584 ## F2 =~ ## Y1 1.000 1.413 0.803 ## Y2 6.793 0.244 27.820 0.000 9.597 0.763 ## Y3 1.456 0.067 21.775 0.000 2.057 0.554 ## Y4 0.280 0.012 23.360 0.000 0.395 0.596 ## ## Regressions: ## Estimate Std.Err z-value P(\u0026gt;|z|) Std.lv Std.all ## F1 ~ ## F2 -0.228 0.025 -9.151 0.000 -0.270 -0.270 ## ## Variances: ## Estimate Std.Err z-value P(\u0026gt;|z|) Std.lv Std.all ## .X1 1.933 0.077 24.991 0.000 1.933 0.574 ## .X2 2.440 0.088 27.650 0.000 2.440 0.708 ## .X3 1.232 0.051 24.071 0.000 1.232 0.539 ## .X4 2.140 0.085 25.186 0.000 2.140 0.582 ## .X5 1.783 0.069 25.906 0.000 1.783 0.614 ## .X6 2.318 0.086 26.801 0.000 2.318 0.659 ## .Y1 1.101 0.067 16.454 0.000 1.101 0.355 ## .Y2 66.303 3.414 19.423 0.000 66.303 0.419 ## .Y3 9.577 0.347 27.610 0.000 9.577 0.694 ## .Y4 0.284 0.011 26.758 0.000 0.284 0.645 ## .F1 1.328 0.094 14.162 0.000 0.927 0.927 ## F2 1.996 0.109 18.251 0.000 1.000 1.000  ","date":1540339200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540339200,"objectID":"83d2481e7fbc72daf9ff06ab4b8b059d","permalink":"/post/structural-equation-modeling-in-r/","publishdate":"2018-10-24T00:00:00Z","relpermalink":"/post/structural-equation-modeling-in-r/","section":"post","summary":"lavaan: A Package for SEM Syntax for Modelling Model Fitting   lavaan: A Package for SEM In my pervious post I made a short introduction to Structural Equation Modeling (SEM). But now it’s about using R for SEM. However, to use R for SEM we need the package lavaan. For the visualization of the models we also need the package semPlot. As you probably know already the installation of the packages is very simple:","tags":["SEM","Structural Equation Modeling","lavaan","semplot","fit indices"],"title":"Structural Equation Modeling in R","type":"post"},{"authors":null,"categories":["R","Tutorial"],"content":"Introduction \u0026amp; Advantages There are a lot of variations regarding Structural Equation Modeling (SEM). Therefore, this article is focusing on the basics of SEM. Fundamentally, SEM can be classified as a combination of path analysis and confirmatory factor analysis. With SEM and path analysis you will have a big felxibility for specifying relationships between theoretical constructs. For example, it is possible to postulate rather complex models that may include a single construct that is theorized to be a predictor of some constructs and is also predicted by other constructs. You can estimate the multiple and interrelated dependence in a single analysis. Moreover, there are two types of variables which are used in SEM. Variables which are used only as a predictor are classified ad exogenous, whereas variables that are predicted by at least one other variable are classified as endogenous. But what are the exact benefits of SEM? Regarding this question, two main advantages of SEM will be discussed below. Latent Variables SEM distinguished between measurement and latent variables. One major difference between SEM and other methods is the use of latent variables which are captured by measurement variables. Latent variables represent constructs that can not be observed. For example, constructs such as stability, well-being, statisfaction or xenophobia are too complex to be measured directly. Moreover, with SEM the measruement error which cannot be explained by the latent variable will be considered. So, in a measurement model you will have a measurement error for each measurement variable. If \\(F\\) is the latent variable and \\(X1\\) and \\(X2\\) are the measurement variables and there is no measurement error, the path of \\(F \\rightarrow X1\\) and \\(F \\rightarrow X2\\) would be \\(1.0\\). But as we now, that will be not the case. So, assume a path of \\(.7\\) for \\(F \\rightarrow X1\\) and \\(.8\\) for \\(F \\rightarrow X2\\). Consequently, there would be a correlation of \\(.56\\) between the two variables \\(X1\\) and \\(X2\\). The measurement error will take into account the difference between \\(1.0\\) and \\(.56\\). Because of this, the relationships between the latent constructs can be more accurate.  Simultaneous Estimation Regarding most methods, only the relationship of a dependent variable to one or more independent variables can be estimated. However, using SEM, it is also possible to estimate the relationship between multiple dependent variables. But the simultaneous estimation gives another advantage. Thus, in addition to the direct effect, the indirect and total effects can be easily calculated. Just a small example: \\(\\small Ind. \\ Effect_{X1\\rightarrow Y} = Dir. \\ Effect_{X1\\rightarrow X2}*Dir. \\ Effect_{X2\\rightarrow Y} = .9*-.47 = -.43\\) \\(\\small Tot. \\ Effect_{X1\\rightarrow Y} = Ind. \\ Effect_{X1\\rightarrow Y} + Dir. \\ Effect_{X1\\rightarrow Y} = -.43 -.42 = -.85\\)\n  Identification Generally, a structural equation model must be identified, otherwise a clear parameter estimation can not be made. The identification should be determined for all model levels:  Identification of the individual measurement models Identification of the structural model Identification of the overall model  The aim should be that there is over-identification at all model levels. In order to determine the degree of identification of a model, the following formula can be used (counting rule): \\[t \\leq\\frac{p(p+1)}{2}\\\\ p = observed \\ variables \\\\ t = estimated\\ paramters\\]\nThe same formula can be used to calculate the degree of freedoms as well:\n\\[df =\\frac{p(p+1)}{2} - t\\]\nConsequently, there are three scenarios for identification:\n \\(df \u0026lt; 0\\): The model is underidentified \\(df = 0\\): The model is exactly identified \\(df \u0026gt; 0\\): The model is overidentified  As mentioned above, the goal should be an overidentified model. This is needed to perform further tests (for example: testing the fit of the model).  Fit Indices At the beginning of SEM, testing the following null hypothesis was considered a good way to assess the model specification: \\[\\sum = \\sum(0)\\]\nWhere \\(\\sum\\) stands for the population covariance matrix and \\(\\sum{0}\\) for the covariance matrix implied by the specified model. To check this null hypothesis, a \\(\\chi^2\\)-test can be used. However, this test is extremely sensitive to high sample sizes1. For this reason, other methods have been developed to test the qualtiy of a model. In the following, some important Fit Indices will be discussed. \\(\\chi^2\\)-Test Despite the high sensitivity with high sample sizes, I would nevertheless explain the test briefly. The \\(\\chi^2\\) is calculated as follows, where O represents the observations and E represents the expected values: \\[\\chi^{2} = \\sum_{i=1}^{n}\\frac{(O_{i}-E_{i})^2}{E_{i}}\\]\nSince the null hypothesis should not be rejected, the test should not be significant. If it comes to a significant result, then the model would have to be rejected or optimized. But as already pointed out, with larger sample size, there will always be a significant result. Based on this test, most models would have to be rejected. For that reason you should not rely on this test.  CFI - Comparative Fit Index In contrast to the \\(\\chi^2\\)-test, the CFI takes into account the sample size and is reliable even for small samples: \\[CFI = 1-\\bigg(\\frac{\\lambda_{k}}{\\lambda_{i}}\\bigg) = 1 - \\bigg(\\frac{max[(\\chi^2_{t}-df_{t}), 0]}{max[(\\chi^2_{t}-df_{t}),(\\chi^2_{n}-df_{n}),0]}\\bigg)\\]\n\\(\\chi^2_{t}\\) and \\(\\chi^2_{n}\\) are the \\(\\chi^2\\) statistics for the target and the baseline model. The degree of freedoms of the target and the null model are represented by \\(df_{t}\\) and \\(df_{n}\\). However, the CFI evaluates the extent to which the tested model is superior to a alternative model in reproducing the observed covariance matrix. The range of the CFI is between \\(0\\) and \\(1\\), whereas a value of \\(1\\) indicates a perfect fit. The cut off value is \\(.95\\)2.  RMSEA - Root Mean Square Error of Approximation RMSEA expresses the discrepancy between the observed covariance matrix and the covariance matrix implied by the model per degree of freedom: \\[RMSEA = \\sqrt\\frac{\\hat{F_{0}}}{df_{t}}=\\sqrt\\frac{max\\Big[\\frac{(\\chi^2_{t}-df_{t})}{(N-1)},0\\Big]}{df_{t}}= \\sqrt\\frac{\\chi^2_{t} - df_{t}}{df_{t}(N-1)}\\]\nWhere \\(\\chi^2_{t}\\) is the \\(\\chi^2\\) statistic and \\(df_{t}\\) the degree of freedom for the target model. In contrast to the CFI, the RMSEA does not perform well with small sample sizes. So, it tends to reject a true model, if the sample size is small. However, the RMSEA does include the model complexity. As with the CFI, there is a range from \\(0\\) to \\(1\\), where a value of \\(0\\) implies a perfect fit. A value of \\(\\leq.05\\) implies a very good fit, a value between \\(.05\\) and \\(.08\\) is considered with a good fit and a value between \\(.08\\) and \\(.10\\) represents a acceptable fit. However, if the RMSEA is greater than \\(.10\\) the fit of the model is bad2.  SRMR - Standardized Root Mean Square Residual The SRMR is the root of the difference between the residuals of the covaruance matrix implied by the specified model and the3: \\[SRMR = \\sqrt{\\frac{\\Bigg\\{2\\sum_{i=1}^{p}\\sum_{j=1}^{i}\\Bigg[\\frac{(s_{ij}-\\hat\\sigma_{ij})}{(s_{ii}*s_{jj})}\\Bigg]^2\\Bigg\\}}{p(p+1)}}\\]\n\\(s_{ii}\\) and \\(s_{jj}\\) are the observed standard deviations. \\(\\sigma_{ij}\\) is the covariance matrix implied by the model, wheras \\(s_{ij}\\) is the observed covariance matrix. A value of \\(0\\) implies a perfect fit. The SRMR should not be greater than \\(.08\\)2.   Model Optimization The modification indices can be used to identify potential model misspecification. If the standardized estimated parameter change (SEPC) is \\(\\geq.20\\), there may be a possible misspecification4. If you want to optimize the model, the parameters can be freely estimated, in which the SEPC is \\(\\geq.20\\). However, this quickly results in parameters being freely estimated where theoretical justification is only partially possible. Thus, error correlations are often freely estimated, which are theoretically rarely well justified5. The fit of the model can indeed be easily optimized in this way. But what brings a better fit, if the resulting model is no longer similar to the theory to be tested? Right, such a model offers little added value. Therefore, a model should only be optimized if this is theoretically well justified.  References 1 Cangur, S. and Ercan, I. 2015: Comparison of model fit indices used in structural equation modeling under multivariate normality. Journal of Modern Applied Statistical Methods. 14 (1), 14.  2 Hu, L.-t. and Bentler, P.M. 1999: Cutoff criteria for fit indexes in covariance structure analysis: Conventional criteria versus new alternatives. Structural Equation Modeling: A Multidisciplinary Journal. 6 (1), 1–55.  3 Chen, F.F. 2007: Sensitivity of goodness of fit indexes to lack of measurement invariance. Structural Equation Modeling. 14 (3), 464–504.  4 Whittaker, T.A. 2012: Using the modification index and standardized expected parameter change for model modification. The Journal of Experimental Education. 80 (1), 26–44.  5 Hermida, R. 2015: The problem of allowing correlated errors in structural equation modeling: Concerns and considerations. Computational Methods in Social Sciences. 3 (1), 5.    ","date":1539388800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539388800,"objectID":"a046aa2becb3418b92874f13d1748b56","permalink":"/post/structural-equation-modeling-a-short-introduction/","publishdate":"2018-10-13T00:00:00Z","relpermalink":"/post/structural-equation-modeling-a-short-introduction/","section":"post","summary":"Introduction \u0026amp; Advantages There are a lot of variations regarding Structural Equation Modeling (SEM). Therefore, this article is focusing on the basics of SEM. Fundamentally, SEM can be classified as a combination of path analysis and confirmatory factor analysis. With SEM and path analysis you will have a big felxibility for specifying relationships between theoretical constructs. For example, it is possible to postulate rather complex models that may include a single construct that is theorized to be a predictor of some constructs and is also predicted by other constructs.","tags":["Structural Equation Modeling","SEM","fit indices","lavaan","semplot"],"title":"Structural Equation Modeling: A Short Introduction","type":"post"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;\n","date":1536444000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536444000,"objectID":"6a451186c775f5f0adb3a0416d0cb711","permalink":"/tutorial/example/","publishdate":"2018-09-09T00:00:00+02:00","relpermalink":"/tutorial/example/","section":"tutorial","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;","tags":null,"title":"Example Page","type":"docs"}]