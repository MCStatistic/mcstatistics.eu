[{"authors":null,"categories":["R"],"content":" Packages \u0026amp; Data Source Data Acquisition Word Matching Supervised Machine Learning Results Sentiment Analysis   For my studies I wrote a seminar paper about the agenda setting of the opposition parties in the German Bundestag. To analyze the agenda setting of the opposition parties, the minor interpellations were used. In the following I would like to show on the one hand the data acquisition and on the other hand the basic aspects of the evaluation. In addition, I will build a Shiny App, so you can look at the data in more detail.\nCode Details can be found on GitHub.\nPackages \u0026amp; Data Source Some packages were used for the analysis. I guess most packages do not need an explanation. However, purrr and downloader were used for downloading all documents of the 18th and 19th Bundestag. For the classification process in terms of supervised machine learning RTextTools was used.\npacman::p_load(dplyr, haven, readr, purrr, ggplot2, downloader, pdftools, stringr, ggthemes, lubridate, tm, RTextTools, forcats, SentimentAnalysis, magrittr) All documents for the German Bundestag are are freely accessible. The source of downloaded data can be found here: DIP (Dokumentations- und Informationssystem für Parlamentarische Vorgänge). Each PDF-Document has its own URL and of course, there is a quite simple pattern which you can use. The pattern can best be shown by two URLs:\nThe first Document of the 19th Bundestag: http://dipbt.bundestag.de/dip21/btd/19/000/1900001.pdf\nThe 101th Document of the 19th Bundestag: http://dipbt.bundestag.de/dip21/btd/19/001/1900101.pdf\nTo produce all the needed URLs I replicated the static components of the URLS so many times as needed. The varying part of the URL consists of a simple numbering in two places of it. Therefore, the pattern was reprocuded and also replicated n-times depending on the logic of the numbering.\n# URLs for the 18th legislature urls_18 \u0026lt;- str_c(rep(\u0026quot;http://dipbt.bundestag.de/dip21/btd/18/\u0026quot;, each = 13700), as.character(sprintf(\u0026quot;%03d\u0026quot;, 000:136)) %\u0026gt;% rep(each = 100), rep(\u0026quot;/18\u0026quot;, each = 13700), as.character(sprintf(\u0026quot;%05d\u0026quot;, 00000:13699)), rep(\u0026quot;.pdf\u0026quot;, each = 13700), spe = \u0026quot;\u0026quot;) %\u0026gt;% .[!. == \u0026quot;http://dipbt.bundestag.de/dip21/btd/18/000/1800000.pdf\u0026quot;] # URLs for the 19th legislature urls_19 \u0026lt;- str_c(rep(\u0026quot;http://dipbt.bundestag.de/dip21/btd/19/0\u0026quot;, each = 6700), as.character(sprintf(\u0026quot;%02d\u0026quot;, 00:66)) %\u0026gt;% rep(each = 100) , rep(\u0026quot;/190\u0026quot;, each = 6700), as.character(sprintf(\u0026quot;%04d\u0026quot;, 0000:6699)), rep(\u0026quot;.pdf\u0026quot;, each = 6700), sep = \u0026quot;\u0026quot;) %\u0026gt;% .[!. == \u0026quot;http://dipbt.bundestag.de/dip21/btd/19/000/1900000.pdf\u0026quot;] # for downloading the documents with the downloader package, a file path is needed path_18 \u0026lt;- paste(\u0026quot;./18_btd/\u0026quot;, basename(urls_18), sep = \u0026quot;\u0026quot;) path_19 \u0026lt;- paste(\u0026quot;./19_btd/\u0026quot;, basename(urls_19), sep = \u0026quot;\u0026quot;) # resulting URLs and file paths head(urls_18) %\u0026gt;% kableExtra::kable()    x      http://dipbt.bundestag.de/dip21/btd/18/000/1800001.pdf    http://dipbt.bundestag.de/dip21/btd/18/000/1800002.pdf    http://dipbt.bundestag.de/dip21/btd/18/000/1800003.pdf    http://dipbt.bundestag.de/dip21/btd/18/000/1800004.pdf    http://dipbt.bundestag.de/dip21/btd/18/000/1800005.pdf    http://dipbt.bundestag.de/dip21/btd/18/000/1800006.pdf     head(path_18) %\u0026gt;% kableExtra::kable()    x      ./18_btd/1800001.pdf    ./18_btd/1800002.pdf    ./18_btd/1800003.pdf    ./18_btd/1800004.pdf    ./18_btd/1800005.pdf    ./18_btd/1800006.pdf     Now, the creaeted URLs can be used for downloading all documents. Instead of building a for loop, the map2() function out of the purrr package was used.\n# creating sleep function sleep_down \u0026lt;- function(...) { download(...) Sys.sleep(0.5) } # creating safely function (if something goes wrong) safe_download \u0026lt;- safely(sleep_down) # downloading all files for the 18th Bundestag map_results_18 \u0026lt;- map2(urls_18, path_18, ~safe_download(.x, .y, mode =\u0026quot;wb\u0026quot;)) # downloading all files for the 18th Bundestag map_results_19 \u0026lt;- map2(urls_19, path_19, ~safe_download(.x, .y, mode = \u0026quot;wb\u0026quot;))  Data Acquisition As you can see, I used a For Loop to extract some information. Anyway, the whole thing is very nested and today I would try to realize it differently. Nevertheless, the required information could be extracted in this way. A for loop was also used for extracting the other information. You can find more infromation on my repository SeminarPaper_ASGB.\nparty \u0026lt;- c() for(i in seq_along(path_anfragen_18_clean)){ text_string \u0026lt;- pdf_text(path_anfragen_18_clean[i]) %\u0026gt;% strsplit(\u0026quot;\\n\u0026quot;) %\u0026gt;% unlist() party \u0026lt;- text_string[4:20] a \u0026lt;- str_detect(party, \u0026quot;GRÜNEN\u0026quot;) if(TRUE %in% a){ z \u0026lt;- \u0026quot;Grüne\u0026quot; } else{ b \u0026lt;- str_detect(party, \u0026quot;LINKE\u0026quot;) if(TRUE %in% b){ z \u0026lt;- \u0026quot;Linke\u0026quot; } else{ g \u0026lt;- str_detect(party, \u0026quot;FDP\u0026quot;) if(TRUE %in% g){ z \u0026lt;- \u0026quot;FDP\u0026quot; } else{ h \u0026lt;- str_detect(party, \u0026quot;AfD\u0026quot;) if(TRUE %in% h){ z \u0026lt;- \u0026quot;AfD\u0026quot; }else{ c \u0026lt;- str_detect(party, \u0026quot;SPD\u0026quot;) if(TRUE %in% c){ z \u0026lt;- \u0026quot;SPD\u0026quot; } else{ d \u0026lt;- str_detect(party, \u0026quot;CDU/CSU\u0026quot;) if(TRUE %in% d){ z \u0026lt;- \u0026quot;Union\u0026quot; } else{ z \u0026lt;- \u0026quot;NA\u0026quot; } } } } } } party \u0026lt;- c(party, z) }  Word Matching After cleaning title and content (e.g. numbers, stopwords, punctuations) we can start with the categorization of the documents. Two methods were used. First of all, a simple word-matching with topic-specific words was made. For each topic a vector of words were created. Furthermore, overlapping matches were detected and corrected. Finally, supervised machine learning was used (Random Forest and Support Vector Machines).\n# labeling the topic with word matching df_complete_clean %\u0026lt;\u0026gt;% mutate(topic = case_when( str_detect(title, makroöko) ~ \u0026quot;Macroeconomics\u0026quot;, str_detect(title, buerger) ~ \u0026quot;Civil Rights\u0026quot;, str_detect(title, gesundheit) ~ \u0026quot;Health\u0026quot;, str_detect(title, agrar) ~ \u0026quot;Agriculture\u0026quot;, str_detect(title, arbeit) ~ \u0026quot;Labor\u0026quot;, str_detect(title, bildung) ~ \u0026quot;Education\u0026quot;, str_detect(title, umwelt) ~ \u0026quot;Environment\u0026quot;, str_detect(title, energie) ~ \u0026quot;Energy\u0026quot;, str_detect(title, einwanderung) ~ \u0026quot;Immigration\u0026quot;, str_detect(title, transport) ~ \u0026quot;Transportation\u0026quot;, str_detect(title, krimi) ~ \u0026quot;Law and Crime\u0026quot;, str_detect(title, sozi) ~ \u0026quot;Social Welfare\u0026quot;, str_detect(title, wohn) ~ \u0026quot;Housing\u0026quot;, str_detect(title, banken) ~ \u0026quot;Domestic Commerce\u0026quot;, str_detect(title, verteidigung) ~ \u0026quot;Defense\u0026quot;, str_detect(title, tech_kom) ~ \u0026quot;Technology\u0026quot;, str_detect(title, außenh) ~ \u0026quot;Foreign Trade\u0026quot;, str_detect(title, int_aus) ~ \u0026quot;International Affairs\u0026quot;, str_detect(title, regierung) ~ \u0026quot;Government Operations\u0026quot;, TRUE ~ \u0026quot;Other\u0026quot; )) After the word matching and the correction of some overlaps, this results in the following topic distribution.\ndf_english %\u0026gt;% filter(label_eng != \u0026quot;Other\u0026quot;) %\u0026gt;% mutate(label_eng_f = as.factor(label_eng)) %\u0026gt;% group_by(label_eng_f) %\u0026gt;% filter(!is.na(label_eng_f)) %\u0026gt;% summarise(n = n()) %\u0026gt;% mutate(perc = (n/sum(n))) %\u0026gt;% mutate(label_eng_f = fct_reorder(label_eng_f, perc, .desc = F)) %\u0026gt;% ggplot(aes(x = label_eng_f, y = perc)) + geom_bar(position = \u0026quot;dodge\u0026quot;, stat = \u0026quot;identity\u0026quot;, fill = \u0026quot;skyblue4\u0026quot;, alpha = 0.9) + geom_text(aes(y = perc, label = sprintf(\u0026quot;%1.1f%%\u0026quot;, round(100*perc, 1))), size = 3.1, color=rgb(100,100,100, maxColorValue=255), hjust=-0.1) + ggtitle(\u0026quot;\u0026quot;) + ylab(\u0026quot;\u0026quot;) + xlab(\u0026quot;\u0026quot;) + theme_bw() + theme(text=element_text(size = 12, family = \u0026quot;LM Roman 10\u0026quot;)) + scale_y_continuous(labels = scales::percent_format(), limits=c(0,0.23)) + coord_flip() It can be seen that the distribution is very unbalanced and some topics are hardly represented. This point should in principle be taken into account when using Random Forest. For example, with strongly unbalanced data, the major class can be down-sampled or the minor class over-sampled.\n Supervised Machine Learning The package RTextTools was used for using Random Forest and Support Vector Machines. Before the final classification, a text classification was performed to test the performance of both methods and to determine the best parameters. For this purpose, a container was created, which contains only the already classified requests. In addition, the requests were split into test and training data. For Random Forest, 200 ntrees were ultimately used and for Support Vector Machines the C-Parameter was set to 10.\n# creating training set df_training_eng \u0026lt;- df_english %\u0026gt;% filter(thema != \u0026quot;Other\u0026quot;) %\u0026gt;% sample_n(3047) # creating document-term matrix df_content_matrix \u0026lt;- create_matrix(df_training_eng$content) # remove sparse terms df_content_matrix \u0026lt;- removeSparseTerms(df_content_matrix, sparse = .99) # creating a container container_eng \u0026lt;- create_container(df_content_matrix, df_training_eng$thema, trainSize=1:2300, testSize=2301:3047, virgin=FALSE) # Support Vector Machines svm \u0026lt;- train_model(container_eng,\u0026quot;SVM\u0026quot;, kernel = \u0026quot;linear\u0026quot;, cost = 10) svm_classify \u0026lt;- classify_model(container_eng, svm) # data preparation for plotting df_svm_eng \u0026lt;- svm_classify %\u0026gt;% mutate(model = as.factor(\u0026quot;Support Vector Machine\u0026quot;)) %\u0026gt;% rename(prob = SVM_PROB, label = SVM_LABEL) # Random Forest rf_200 \u0026lt;- train_model(container_eng, \u0026quot;RF\u0026quot;, ntree = 200) rf_classify_200 \u0026lt;- classify_model(container_eng, rf_200) # data preparation for plotting df_rf_200_eng \u0026lt;- rf_classify_200 %\u0026gt;% mutate(model = as.factor(\u0026quot;Random Forest\u0026quot;)) %\u0026gt;% rename(prob = FORESTS_PROB, label = FORESTS_LABEL) prob_plot_df \u0026lt;- rbind.data.frame(df_rf_200_eng, df_svm_eng) Now, we can check the class probabilities. The class probabilities were also used for the final classification to ensure the most reliable classification possible.\nprob_plot_df %\u0026gt;% group_by(model) %\u0026gt;% mutate(label_eng = fct_reorder(label_eng, prob)) %\u0026gt;% ggplot(aes(x = label_eng, y = prob, fill = label_eng)) + geom_boxplot(alpha = 0.2) + geom_jitter(width = 0.2, alpha = 0.2) + theme_bw() + xlab(\u0026quot;\u0026quot;) + ylab(\u0026quot;Class Probabilities\u0026quot;) + facet_wrap(~ model) + theme(legend.position = \u0026quot;none\u0026quot;, text=element_text(size = 12, family = \u0026quot;LM Roman 10\u0026quot;)) + coord_flip()  As you can see, the performance of both algorithms is not good on every topic. Therefore, cut off values for the class probability were used for the final classification.\n Results Now, we can take a look to the results. However, I will keep it short regarding the interpretation though.\ndf_english %\u0026gt;% filter(party != \u0026quot;NA\u0026quot;) %\u0026gt;% mutate(date_n = as.Date(date, \u0026quot;%d.%m.%Y\u0026quot;)) %\u0026gt;% mutate(year = year(date_n), party = case_when(party == \u0026quot;Union\u0026quot; ~ \u0026quot;CDU/CSU\u0026quot;, TRUE ~ party)) %\u0026gt;% mutate(year = as.character(year)) %\u0026gt;% group_by(party, year, bt_f) %\u0026gt;% summarise(n = n()) %\u0026gt;% ggplot(aes(x = as.factor(year), fill = reorder(party, -n), y = n)) + geom_bar(position=\u0026quot;dodge\u0026quot;, stat=\u0026quot;identity\u0026quot;, alpha = 0.9) + geom_text(position = position_dodge(width = 1), aes(label = n), vjust = -0.5, size = 3) + xlab(\u0026quot;\u0026quot;) + ylab(\u0026quot;\u0026quot;) + labs(fill = \u0026quot;\u0026quot;) + theme_bw() + theme(legend.position=\u0026quot;bottom\u0026quot;, text=element_text(size = 12, family = \u0026quot;LM Roman 10\u0026quot;)) + scale_fill_manual(values = c(\u0026quot;Grüne\u0026quot; = \u0026quot;#50822E\u0026quot;, \u0026quot;Linke\u0026quot; = \u0026quot;#B61C3E\u0026quot;, \u0026quot;CDU/CSU\u0026quot; = \u0026quot;#32372C\u0026quot;, \u0026quot;SPD\u0026quot; = \u0026quot;#E3000F\u0026quot;, \u0026quot;AfD\u0026quot; = \u0026quot;#0088FF\u0026quot;, \u0026quot;FDP\u0026quot; = \u0026quot;#FFD600\u0026quot;)) + facet_wrap(~ bt_f, strip.position = \u0026quot;top\u0026quot;, dir = \u0026quot;v\u0026quot;) + scale_y_continuous(limits = c(0,800)) There are no significant differences in the number of minor interpellations. Only Die Linke always has more minor interpellations than the other parties. And, of course, it is clear that the minor interpellations are above all a means of opposition. The governing parties almost do not use this instrument at all.\ndf_english %\u0026gt;% filter(label_eng != \u0026quot;Other\u0026quot;) %\u0026gt;% mutate(label_eng_f = as.factor(label_eng)) %\u0026gt;% group_by(bt_f, party, label_eng_f) %\u0026gt;% filter(party != \u0026quot;SPD\u0026quot; \u0026amp; party != \u0026quot;Union\u0026quot; \u0026amp; party != \u0026quot;NA\u0026quot;) %\u0026gt;% filter(!is.na(label_eng_f)) %\u0026gt;% summarise(n = n()) %\u0026gt;% mutate(perc = (n/sum(n))) %\u0026gt;% mutate(label_eng_f = fct_reorder2(label_eng_f, party, perc, .desc = F)) %\u0026gt;% ggplot(aes(x = label_eng_f, y = perc, fill = party)) + geom_bar(position = \u0026quot;dodge\u0026quot;, stat = \u0026quot;identity\u0026quot;, alpha = 0.9) + scale_fill_manual(values = c(\u0026quot;Grüne\u0026quot; = \u0026quot;#50822E\u0026quot;, \u0026quot;Linke\u0026quot; = \u0026quot;#B61C3E\u0026quot;, \u0026quot;CDU/CSU\u0026quot; = \u0026quot;#32372C\u0026quot;, \u0026quot;SPD\u0026quot; = \u0026quot;#E3000F\u0026quot;, \u0026quot;AfD\u0026quot; = \u0026quot;#0088FF\u0026quot;, \u0026quot;FDP\u0026quot; = \u0026quot;#FFD600\u0026quot;)) + xlab(\u0026quot;\u0026quot;) + ylab(\u0026quot;\u0026quot;) + labs(fill = \u0026quot;\u0026quot;) + theme_bw() + theme(text=element_text(size = 12), legend.position=\u0026quot;bottom\u0026quot;) + scale_y_continuous(labels = scales::percent_format()) + facet_wrap(~bt_f) + coord_flip() This plot shows the percentage of each topic for each party. You can clearly see that almost every party has a clear thematic focus. Immigration has the highest share of the minor interpolations of the AfD. By contrast, law and crime makes up the largest share of Die Linke. Among Grüne, transport makes up the biggest share, with a focus on e-mobility. The FDP also has the largest share in transport, but it is lower than in the Grünen. In general, the FDP has no such clear focus on a specific topic as the other parties do. In contrast, the FDP is moderately represented on some topics.\n Sentiment Analysis As a small addition, a sentiment analysis was also performed. For this, the sentiment for each minor interpolation was determined based on the content. Subsequently, an averaging comparison was made between the parties. The dictionary, SentiWS, of the University of Leipzig was used.\n# creating two types of dictionaries senti_dict_binary \u0026lt;- SentimentDictionaryBinary(pos_df$Wort, neg_df$Wort) # splitting df because memory can\u0026#39;t handle the amount of data :( df_sonst_f \u0026lt;- df_complete_clean_f %\u0026gt;% filter(label == \u0026quot;Sonstiges\u0026quot;) df_lab_f \u0026lt;- df_complete_clean_f %\u0026gt;% filter(label != \u0026quot;Sonstiges\u0026quot;) # sentiment analysis for the two splitted parts of the df df_lab_f$sentiment \u0026lt;- analyzeSentiment(df_lab_f$content, rules=list(\u0026quot;Amplifiers\u0026quot;=list(ruleSentiment, senti_dict_binary)))[,1] df_sonst_f$sentiment \u0026lt;- analyzeSentiment(df_sonst_f$content, rules=list(\u0026quot;Amplifiers\u0026quot;=list(ruleSentiment, senti_dict_binary)))[,1] # combining splitted df with sentiment-scores df_complete_fin \u0026lt;- rbind(df_lab_f, df_sonst_f) # sentiment analysis for the two splitted parts of the df df_lab_f$sentiment2 \u0026lt;- analyzeSentiment(df_lab_f$content, rules=list(\u0026quot;Amplifiers\u0026quot;=list(ruleSentimentPolarity, senti_dict_binary)))[,1] df_sonst_f$sentiment2 \u0026lt;- analyzeSentiment(df_sonst_f$content, rules=list(\u0026quot;Amplifiers\u0026quot;=list(ruleSentimentPolarity, senti_dict_binary)))[,1] comparisons \u0026lt;- list( c(\u0026quot;AfD\u0026quot;, \u0026quot;Linke\u0026quot;), c(\u0026quot;AfD\u0026quot;, \u0026quot;Grüne\u0026quot;), c(\u0026quot;AfD\u0026quot;, \u0026quot;FDP\u0026quot;), c(\u0026quot;FDP\u0026quot;, \u0026quot;Linke\u0026quot;), c(\u0026quot;FDP\u0026quot;, \u0026quot;Grüne\u0026quot;), c(\u0026quot;Grüne\u0026quot;, \u0026quot;Linke\u0026quot;)) df_english %\u0026gt;% filter(party != \u0026quot;SPD\u0026quot; \u0026amp; party != \u0026quot;Union\u0026quot;) %\u0026gt;% filter(label_eng == \u0026quot;Law and Crime\u0026quot; | label_eng == \u0026quot;Immigration\u0026quot; | label_eng == \u0026quot;Environment\u0026quot; | label_eng == \u0026quot;Transportation\u0026quot; | label_eng == \u0026quot;Defense\u0026quot; | label_eng == \u0026quot;Energy\u0026quot;) %\u0026gt;% group_by(label_eng, party) %\u0026gt;% ggplot(., aes(x = party, y = sentiment, col = party)) + geom_boxplot(alpha = 1, width = 0.2) + geom_violin(alpha = 0.3) + labs(fill =\u0026quot;\u0026quot;, color = \u0026quot;\u0026quot;) + ggpubr::stat_compare_means(comparisons = comparisons, bracket.size = 0.3, size = 3, text = element_text(family = \u0026quot;LM Roman 10\u0026quot;)) + scale_color_manual(values = c(\u0026quot;Grüne\u0026quot; = \u0026quot;#50822E\u0026quot;, \u0026quot;Linke\u0026quot; = \u0026quot;#B61C3E\u0026quot;, \u0026quot;CDU/CSU\u0026quot; = \u0026quot;#32372C\u0026quot;, \u0026quot;SPD\u0026quot; = \u0026quot;#E3000F\u0026quot;, \u0026quot;AfD\u0026quot; = \u0026quot;#0088FF\u0026quot;, \u0026quot;FDP\u0026quot; = \u0026quot;#FFD600\u0026quot;)) + theme_bw() + theme(text=element_text(size = 12, family = \u0026quot;LM Roman 10\u0026quot;), plot.margin=grid::unit(c(0,0,0,0), \u0026quot;mm\u0026quot;), axis.title.x=element_blank(), axis.title.y = element_blank(), legend.position = \u0026quot;bottom\u0026quot;) + facet_wrap(~label_eng)   ","date":1551657600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551657600,"objectID":"38babca46c82c0be073ee824690d045f","permalink":"/post/german-bundestag-minor-interpellations/","publishdate":"2019-03-04T00:00:00Z","relpermalink":"/post/german-bundestag-minor-interpellations/","section":"post","summary":"Packages \u0026amp; Data Source Data Acquisition Word Matching Supervised Machine Learning Results Sentiment Analysis   For my studies I wrote a seminar paper about the agenda setting of the opposition parties in the German Bundestag. To analyze the agenda setting of the opposition parties, the minor interpellations were used. In the following I would like to show on the one hand the data acquisition and on the other hand the basic aspects of the evaluation.","tags":["R","Analysis","sentiment Analysis"],"title":"German Bundestag - Minor Interpellations ","type":"post"},{"authors":null,"categories":["R","Tutorial"],"content":" Introduction to dplyr filter() select() function mutate() function arrange() function group_by() \u0026amp; summarise() function   library(rmdformats) Introduction to dplyr When it comes to data manipulation, dplyr is a very powerful package with very intuitive functions. There are six important functions you will use often, if it comes to data manipulation:  filter() select() mutate() arrange() group_by() summarise()  In addition to these functions dplyr offers you the ability to use the pipe-operator out of the magrittr package. This operator is very useful and I highly recommend you to get used to it. Except these functions, dplyr offers a lot more functions which are very useful. And, like always, you should look into the reference manual of dplyr for more details! I will mostly use the data frame gapminder for explaining the functions.  filter() In many cases you need to filter your dataset because you are only interested in specific cases or you want to exclude some cases. For that, the filter() function provides an easy way to do that. Let’s take a look into the gapminder df, which is an excerpt of the Gapminder data on life expectancy, GDP per capita, and population by country:\nlibrary(dplyr) library(gapminder) knitr::kable(head(gapminder))   country continent year lifeExp pop gdpPercap    Afghanistan Asia 1952 28.801 8425333 779.4453  Afghanistan Asia 1957 30.332 9240934 820.8530  Afghanistan Asia 1962 31.997 10267083 853.1007  Afghanistan Asia 1967 34.020 11537966 836.1971  Afghanistan Asia 1972 36.088 13079460 739.9811  Afghanistan Asia 1977 38.438 14880372 786.1134    Now, different aspects of the data may be of interest:\n data for specific year(s) data for specific continent(s) data for specific continent(s) and year(s)  A very simple filtering could look like this one:\n# filtering by the year 1997 gapminder %\u0026gt;% filter(year == 1997) %\u0026gt;% head() %\u0026gt;% kableExtra::kable()    country  continent  year  lifeExp  pop  gdpPercap      Afghanistan  Asia  1997  41.763  22227415  635.3414    Albania  Europe  1997  72.950  3428038  3193.0546    Algeria  Africa  1997  69.152  29072015  4797.2951    Angola  Africa  1997  40.963  9875024  2277.1409    Argentina  Americas  1997  73.275  36203463  10967.2820    Australia  Oceania  1997  78.830  18565243  26997.9366     It is also possible to filter with more than one condition. If we are only interested in the year 1997 and the continent asia, we could use the following code. # filtering by the year 1997 and the continent asia gapminder %\u0026gt;% filter(year == 1997 \u0026amp; continent == \u0026quot;Asia\u0026quot;) %\u0026gt;% head() %\u0026gt;% kableExtra::kable()    country  continent  year  lifeExp  pop  gdpPercap      Afghanistan  Asia  1997  41.763  22227415  635.3414    Bahrain  Asia  1997  73.925  598561  20292.0168    Bangladesh  Asia  1997  59.412  123315288  972.7700    Cambodia  Asia  1997  56.534  11782962  734.2852    China  Asia  1997  70.426  1230075000  2289.2341    Hong Kong, China  Asia  1997  80.000  6495918  28377.6322     As you can imagine, there are many logical operators that you can use. I guess, most of you know these operators already.  \u0026lt;: less than \u0026lt;=: less than or equal to ==: equal to !=: not equal to \u0026gt;=: greater than or eqal to \u0026gt;: greater than %in%: matching argument !x: not x (e.g. often used for NA exclusion !is.na(variable))  The following operators can be used to combine different conditions:\n \u0026amp;: filtering by condition 1 and contiditon 2 |: filtering by condition 1 or condition 2  You will see a few of these operators in action later.\n select() function The select() function is a very simple one but can be useful, if the data frame is very large but you only need a few variables. You can select specific coloumns and you can also delete sepcific coloumns. # let\u0026#39;s select the coloumns country, continent, year and lifeExp gapminder %\u0026gt;% select(country, continent, year, lifeExp) %\u0026gt;% head() %\u0026gt;% kableExtra::kable()    country  continent  year  lifeExp      Afghanistan  Asia  1952  28.801    Afghanistan  Asia  1957  30.332    Afghanistan  Asia  1962  31.997    Afghanistan  Asia  1967  34.020    Afghanistan  Asia  1972  36.088    Afghanistan  Asia  1977  38.438     # a shorter way to select these coloumns gapminder %\u0026gt;% select(country:lifeExp) %\u0026gt;% head() %\u0026gt;% kableExtra::kable()    country  continent  year  lifeExp      Afghanistan  Asia  1952  28.801    Afghanistan  Asia  1957  30.332    Afghanistan  Asia  1962  31.997    Afghanistan  Asia  1967  34.020    Afghanistan  Asia  1972  36.088    Afghanistan  Asia  1977  38.438     If we want to delete only a few columns of the df, we can do this by using -:\n# deleting the column population gapminder %\u0026gt;% select(-pop) %\u0026gt;% head() %\u0026gt;% kableExtra::kable()    country  continent  year  lifeExp  gdpPercap      Afghanistan  Asia  1952  28.801  779.4453    Afghanistan  Asia  1957  30.332  820.8530    Afghanistan  Asia  1962  31.997  853.1007    Afghanistan  Asia  1967  34.020  836.1971    Afghanistan  Asia  1972  36.088  739.9811    Afghanistan  Asia  1977  38.438  786.1134      mutate() function You can create new columns with the mutate() function. Moreover, it is possible to use the mutate() function in combination with other functions (e.g. case_when()). In the following code I create a new variable, where the value is the log of the population size. gapminder %\u0026gt;% mutate(log_pop = log(pop)) %\u0026gt;% head() %\u0026gt;% kableExtra::kable()    country  continent  year  lifeExp  pop  gdpPercap  log_pop      Afghanistan  Asia  1952  28.801  8425333  779.4453  15.94675    Afghanistan  Asia  1957  30.332  9240934  820.8530  16.03915    Afghanistan  Asia  1962  31.997  10267083  853.1007  16.14445    Afghanistan  Asia  1967  34.020  11537966  836.1971  16.26115    Afghanistan  Asia  1972  36.088  13079460  739.9811  16.38655    Afghanistan  Asia  1977  38.438  14880372  786.1134  16.51555     Now, let’s create a new variable with three categories. Therefore, we will use the mutate() function and the case_when() function in combination. In the following example the new variable should include three categories: low life expectancy (lifeExp \u0026lt; 50), medium life expectancy (lifeExp \u0026gt;= 50 \u0026amp; \u0026lt;70) and high life expectancy (\u0026gt;= 70). gapminder %\u0026gt;% mutate(lifeExp_cat = case_when( lifeExp \u0026lt; 50 ~ \u0026quot;low\u0026quot;, lifeExp \u0026lt; 70 \u0026amp; lifeExp \u0026gt;= 50 ~ \u0026quot;medium\u0026quot;, lifeExp \u0026gt;= 70 ~ \u0026quot;high\u0026quot; )) %\u0026gt;% head() %\u0026gt;% kableExtra::kable()    country  continent  year  lifeExp  pop  gdpPercap  lifeExp_cat      Afghanistan  Asia  1952  28.801  8425333  779.4453  low    Afghanistan  Asia  1957  30.332  9240934  820.8530  low    Afghanistan  Asia  1962  31.997  10267083  853.1007  low    Afghanistan  Asia  1967  34.020  11537966  836.1971  low    Afghanistan  Asia  1972  36.088  13079460  739.9811  low    Afghanistan  Asia  1977  38.438  14880372  786.1134  low     As you can see, you can add a bunch of conditions with the case_when() function. Moreover, with this combination of these functions it is very easy to manipulate specific cases if it’s needed. Personally, I use the combination of these functions very often. In this way, it is very easy to implement specific encodings.  arrange() function This function can be used for sorting the data by one variable. You can also use multiple variables for this function. It is good for a first exploratory analysis or if you want to familiarize yourself with the data. The following code sorts the data by lifeExp: gapminder %\u0026gt;% arrange(lifeExp) %\u0026gt;% head() %\u0026gt;% kableExtra::kable()    country  continent  year  lifeExp  pop  gdpPercap      Rwanda  Africa  1992  23.599  7290203  737.0686    Afghanistan  Asia  1952  28.801  8425333  779.4453    Gambia  Africa  1952  30.000  284320  485.2307    Angola  Africa  1952  30.015  4232095  3520.6103    Sierra Leone  Africa  1952  30.331  2143249  879.7877    Afghanistan  Asia  1957  30.332  9240934  820.8530     If you want to sort your data in descending order instead of ascending order, you can do this by using desc(): gapminder %\u0026gt;% arrange(desc(lifeExp)) %\u0026gt;% head() %\u0026gt;% kableExtra::kable()    country  continent  year  lifeExp  pop  gdpPercap      Japan  Asia  2007  82.603  127467972  31656.07    Hong Kong, China  Asia  2007  82.208  6980412  39724.98    Japan  Asia  2002  82.000  127065841  28604.59    Iceland  Europe  2007  81.757  301931  36180.79    Switzerland  Europe  2007  81.701  7554661  37506.42    Hong Kong, China  Asia  2002  81.495  6762476  30209.02     As mentioned above, it is possible to sort the data by multiple variables. The following code sorts the data by year in ascending order and then by lifeExp in descending order: gapminder %\u0026gt;% arrange(year, desc(lifeExp)) %\u0026gt;% head() %\u0026gt;% kableExtra::kable()    country  continent  year  lifeExp  pop  gdpPercap      Norway  Europe  1952  72.67  3327728  10095.422    Iceland  Europe  1952  72.49  147962  7267.688    Netherlands  Europe  1952  72.13  10381988  8941.572    Sweden  Europe  1952  71.86  7124673  8527.845    Denmark  Europe  1952  70.78  4334000  9692.385    Switzerland  Europe  1952  69.62  4815000  14734.233      group_by() \u0026amp; summarise() function If you take a look into the data for the first time, it is often very useful to make some exploratory data analysis. The group_by() function in combination with the summarise() function makes it easy. For example, we can group by continent, calculate the mean for the life expectancy and plot the results for each continent over the time. I did that all with the help of the pipe operator (%\u0026gt;%). If you are unfamiliar with the use of this operator, you should familiarize yourself with it. In my opinion, you can work much more efficiently with it. In short, the %\u0026gt;%-operator can be seen as a concatenation of functions. library(ggplot2) gapminder %\u0026gt;% group_by(continent, year) %\u0026gt;% summarise(mean_lifeExp = mean(lifeExp)) %\u0026gt;% ggplot(aes(x = year, y = mean_lifeExp, col = continent)) + geom_line() Of course you do not have to plot immediately. However, you can easily extract important data from your dataset. Let’s group again by continent but now we will calculate the mean of life expectancy, the total population size and the median of the GDP for each continent: gapminder %\u0026gt;% filter(year == 1997) %\u0026gt;% group_by(continent) %\u0026gt;% summarise(mean_lifeExp = mean(lifeExp), total_pop = sum(as.numeric(pop)), median_gdp = median(gdpPercap)) %\u0026gt;% head() %\u0026gt;% kableExtra::kable()    continent  mean_lifeExp  total_pop  median_gdp      Africa  53.59827  743832984  1179.883    Americas  71.15048  796900410  7113.692    Asia  68.02052  3383285500  3645.380    Europe  75.50517  568944148  19596.499    Oceania  78.19000  22241430  24024.175      ","date":1542326400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542326400,"objectID":"9ba9ff53b539e626fea400e221010de2","permalink":"/post/data-wrangling-part-1-introduction-to-dplyr/","publishdate":"2018-11-16T00:00:00Z","relpermalink":"/post/data-wrangling-part-1-introduction-to-dplyr/","section":"post","summary":"Introduction to dplyr filter() select() function mutate() function arrange() function group_by() \u0026amp; summarise() function   library(rmdformats) Introduction to dplyr When it comes to data manipulation, dplyr is a very powerful package with very intuitive functions. There are six important functions you will use often, if it comes to data manipulation:  filter() select() mutate() arrange() group_by() summarise()  In addition to these functions dplyr offers you the ability to use the pipe-operator out of the magrittr package.","tags":["dplyr","Tidyverse","data wrangling","data manipulation"],"title":"Data Wrangling Part 1: Introduction to dplyr","type":"post"},{"authors":null,"categories":["R","Tutorial"],"content":" lavaan: A Package for SEM Syntax for Modelling Model Fitting (Standardized) Estimated Parameter Change Visualization: Plotting SEM Sampling Weights   lavaan: A Package for SEM In my pervious post I made a short introduction to Structural Equation Modeling (SEM). But now it’s about using R for SEM. However, to use R for SEM we need the package lavaan and I will introduce the basic functions of the package. For the visualization of the models we also need the package semPlot. As you probably know already the installation of the packages is very simple: install.packages(\u0026quot;lavaan\u0026quot;) install.packages(\u0026quot;semPlot\u0026quot;)  Syntax for Modelling The modely syntax of lavaan is used for specifying a SEM and is also very intuitive. There are three operators for model specification:  =~ is used to specify latent variables/factors ~~ is used to specify (residual) (co)variances ~ is used to specify rgeressions  An example model specification might look like this:\nexample \u0026lt;- \u0026quot; # measurement model F1 =~ 1*V1 + V2 + V3 F2 =~ 1*V4 + V5 + V6 # (residual) (co)variances V1 ~~ V1 # variance V1 ~~ V3 # residual correlation # regressions F1 ~ F2\u0026quot;  Model Fitting Now, let’s specify the first model. I will use an own dataset from a previous work. It’s also possible to use the built-in datasets HolzingerSwineford1939 or PolitcialDemocracy from lavaan to become familiar with the package. Moreover, the functions cfa() and sem() are currently similar but they may differ in the future. It’s also possible to use one of the robust estimators, if you’re fitting the model. library(lavaan) model \u0026lt;- \u0026quot; # measurement model F1 =~ 1*X1 + X2 + X3 + X4 + X5 + X6 F2 =~ 1*Y1 + Y2 + Y3 + Y4 # regression F1 ~ F2\u0026quot; # fitting the model fit \u0026lt;- cfa(model, data = data, estimator = \u0026quot;MLM\u0026quot;) # robust estimator # summary of the fit with standardized values and fit measurements summary(fit, standardized = T, fit.measures = T) ## lavaan 0.6-3 ended normally after 39 iterations ## ## Optimization method NLMINB ## Number of free parameters 21 ## ## Number of observations 1914 ## ## Estimator ML Robust ## Model Fit Test Statistic 218.644 204.758 ## Degrees of freedom 34 34 ## P-value (Chi-square) 0.000 0.000 ## Scaling correction factor 1.068 ## for the Satorra-Bentler correction ## ## Model test baseline model: ## ## Minimum Function Test Statistic 4983.424 4648.582 ## Degrees of freedom 45 45 ## P-value 0.000 0.000 ## ## User model versus baseline model: ## ## Comparative Fit Index (CFI) 0.963 0.963 ## Tucker-Lewis Index (TLI) 0.951 0.951 ## ## Robust Comparative Fit Index (CFI) 0.963 ## Robust Tucker-Lewis Index (TLI) 0.951 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -39038.962 -39038.962 ## Loglikelihood unrestricted model (H1) -38929.641 -38929.641 ## ## Number of free parameters 21 21 ## Akaike (AIC) 78119.925 78119.925 ## Bayesian (BIC) 78236.621 78236.621 ## Sample-size adjusted Bayesian (BIC) 78169.903 78169.903 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.053 0.051 ## 90 Percent Confidence Interval 0.047 0.060 0.045 0.058 ## P-value RMSEA \u0026lt;= 0.05 0.203 0.366 ## ## Robust RMSEA 0.053 ## 90 Percent Confidence Interval 0.046 0.060 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.036 0.036 ## ## Parameter Estimates: ## ## Information Expected ## Information saturated (h1) model Structured ## Standard Errors Robust.sem ## ## Latent Variables: ## Estimate Std.Err z-value P(\u0026gt;|z|) Std.lv Std.all ## F1 =~ ## X1 1.000 1.197 0.652 ## X2 0.839 0.042 20.082 0.000 1.004 0.541 ## X3 0.857 0.040 21.283 0.000 1.026 0.679 ## X4 1.035 0.044 23.642 0.000 1.239 0.646 ## X5 0.884 0.042 20.917 0.000 1.058 0.621 ## X6 0.916 0.045 20.517 0.000 1.096 0.584 ## F2 =~ ## Y1 1.000 1.413 0.803 ## Y2 6.793 0.243 28.008 0.000 9.597 0.763 ## Y3 1.456 0.065 22.322 0.000 2.057 0.554 ## Y4 0.280 0.012 23.170 0.000 0.395 0.596 ## ## Regressions: ## Estimate Std.Err z-value P(\u0026gt;|z|) Std.lv Std.all ## F1 ~ ## F2 -0.228 0.024 -9.345 0.000 -0.270 -0.270 ## ## Variances: ## Estimate Std.Err z-value P(\u0026gt;|z|) Std.lv Std.all ## .X1 1.933 0.083 23.316 0.000 1.933 0.574 ## .X2 2.440 0.082 29.879 0.000 2.440 0.708 ## .X3 1.232 0.059 20.803 0.000 1.232 0.539 ## .X4 2.140 0.086 25.025 0.000 2.140 0.582 ## .X5 1.783 0.076 23.567 0.000 1.783 0.614 ## .X6 2.318 0.088 26.265 0.000 2.318 0.659 ## .Y1 1.101 0.072 15.382 0.000 1.101 0.355 ## .Y2 66.303 3.638 18.223 0.000 66.303 0.419 ## .Y3 9.577 0.369 25.929 0.000 9.577 0.694 ## .Y4 0.284 0.011 26.420 0.000 0.284 0.645 ## .F1 1.328 0.088 15.050 0.000 0.927 0.927 ## F2 1.996 0.093 21.441 0.000 1.000 1.000 If you want to select specific fit measurements, you can use the function fitmeasures(). By using the command fit.measures = \u0026quot;all\u0026quot; you will get all available fit measures. fitmeasures(fit, fit.measures = c(\u0026quot;chisq\u0026quot;, \u0026quot;cfi\u0026quot;, \u0026quot;rmsea\u0026quot;, \u0026quot;srmr\u0026quot;)) %\u0026gt;% kableExtra::kable()     x      chisq  218.64363505    cfi  0.96261082    rmsea  0.05326682    srmr  0.03600232     Apart from the fit indices, the standardized residuals can also be used to interpret the model fit. Unfortunately there is no function to get the standardized residuals. However, a function can be written very fast for this. Using cov2cor on the covariance matrices gives us the fitted and observed correlation matrices. To get the residual correlation matrix we need to subtracting the fitted correlation matrix from the observed correlation matrix. std_residuals \u0026lt;- function(fit) { cov \u0026lt;- list(observed = inspect(fit, \u0026quot;sampstat\u0026quot;)$cov, fitted = fitted(fit)$cov) cor \u0026lt;- list(observed = cov2cor(cov$observed), fitted = cov2cor(cov$fitted)) cor$residual \u0026lt;- cor$observed - cor$fitted lapply(cor, function(x) round(x, 2)) } std_residuals(fit) ## $observed ## X1 X2 X3 X4 X5 X6 Y1 Y2 Y3 Y4 ## X1 1.00 ## X2 0.39 1.00 ## X3 0.43 0.44 1.00 ## X4 0.42 0.30 0.43 1.00 ## X5 0.38 0.30 0.42 0.42 1.00 ## X6 0.41 0.28 0.35 0.43 0.38 1.00 ## Y1 -0.13 -0.10 -0.18 -0.13 -0.18 -0.06 1.00 ## Y2 -0.13 -0.09 -0.16 -0.13 -0.17 -0.08 0.63 1.00 ## Y3 -0.08 -0.08 -0.13 -0.05 -0.13 0.01 0.43 0.39 1.00 ## Y4 -0.12 -0.07 -0.13 -0.10 -0.18 -0.05 0.46 0.43 0.43 1.00 ## ## $fitted ## X1 X2 X3 X4 X5 X6 Y1 Y2 Y3 Y4 ## X1 1.00 ## X2 0.35 1.00 ## X3 0.44 0.37 1.00 ## X4 0.42 0.35 0.44 1.00 ## X5 0.41 0.34 0.42 0.40 1.00 ## X6 0.38 0.32 0.40 0.38 0.36 1.00 ## Y1 -0.14 -0.12 -0.15 -0.14 -0.13 -0.13 1.00 ## Y2 -0.13 -0.11 -0.14 -0.13 -0.13 -0.12 0.61 1.00 ## Y3 -0.10 -0.08 -0.10 -0.10 -0.09 -0.09 0.44 0.42 1.00 ## Y4 -0.10 -0.09 -0.11 -0.10 -0.10 -0.09 0.48 0.45 0.33 1.00 ## ## $residual ## X1 X2 X3 X4 X5 X6 Y1 Y2 Y3 Y4 ## X1 0.00 ## X2 0.04 0.00 ## X3 -0.01 0.07 0.00 ## X4 -0.01 -0.05 -0.01 0.00 ## X5 -0.03 -0.04 0.00 0.02 0.00 ## X6 0.03 -0.04 -0.05 0.05 0.02 0.00 ## Y1 0.01 0.02 -0.03 0.01 -0.05 0.07 0.00 ## Y2 0.01 0.03 -0.02 0.00 -0.04 0.04 0.02 0.00 ## Y3 0.01 0.00 -0.02 0.05 -0.04 0.10 -0.02 -0.03 0.00 ## Y4 -0.01 0.01 -0.03 0.00 -0.08 0.05 -0.02 -0.02 0.10 0.00  (Standardized) Estimated Parameter Change To identify potential model misspecification, you can use modificationindices(). If the SEPC is \\(\\geq.20\\), there may be a possible misspecification. However, adjustments to the model should be based on a theoretical justification. # first 10 rows as output \u0026amp; sorting by mi-value head(modificationindices(fit, sort. = T), 10) %\u0026gt;% dplyr::select(-sepc.lv, -sepc.nox) %\u0026gt;% kableExtra::kable()     lhs  op  rhs  mi  epc  sepc.all      78  Y3  ~~  Y4  59.82491  0.3450967  0.2091268    73  Y1  ~~  Y2  50.65517  4.9004680  0.5736744    43  X2  ~~  X3  44.57299  0.3354141  0.1934529    33  F2  =~  X6  21.73991  0.1452641  0.1094232    53  X3  ~~  X6  20.47615  -0.2289873  -0.1355164    59  X4  ~~  X6  20.35185  0.2900851  0.1302539    44  X2  ~~  X4  20.23462  -0.2881275  -0.1260850    32  F2  =~  X5  16.26564  -0.1121514  -0.0930021    75  Y1  ~~  Y4  14.99285  -0.0978666  -0.1749505    71  X6  ~~  Y3  13.63301  0.4394287  0.0932713      Visualization: Plotting SEM For visualization of an structural equation model the package semPlot offers many possibilities. I would recommend you to look in the reference manual of this package. There are a lot of specifications you can do. However, the specification that I’m using should cover everything necessary for the beginning. In general, an adjustment of the rotation and the layout will probably be necessary. library(semPlot) semPaths(fit, # fitted model nCharNodes = 0, # no abbreviation in node labels whatLabels = \u0026quot;std\u0026quot;, # standardized parameter estimate residuals = F, # excluding residuals and variances sizeLat = 10, # width latent sizeLat2 = 10, # height latent sizeMan = 6, # width manifest edge.label.cex = 0.90, # font size of parameters layout = \u0026quot;tree2\u0026quot;, # type of layout rotation = 2) # rotation of the layout If you want to edit the label of the nodes, you can do this by using the nodeLabels argument. To do so, the order of the nodes must first be determined.\n# determine the order of the nodes semPaths(fit, # fitted model nCharNodes = 0, # no abbreviation in node labels whatLabels = \u0026quot;std\u0026quot;, # standardized parameter estimate residuals = F, # excluding residuals and variances sizeLat = 10, # width latent sizeLat2 = 10, # height latent sizeMan = 6, # width manifest edge.label.cex = 0.90, # font size of parameters layout = \u0026quot;tree2\u0026quot;, # type of layout rotation = 2, # rotation of the layout nodeLabels = letters[1:12]) # getting the order of the nodes # labeling the nodes semPaths(fit, # fitted model nCharNodes = 0, # no abbreviation in node labels whatLabels = \u0026quot;std\u0026quot;, # standardized parameter estimate residuals = F, # excluding residuals and variances sizeLat = 10, # width latent sizeLat2 = 10, # height latent sizeMan = 6, # width manifest edge.label.cex = 0.90, # font size of parameters layout = \u0026quot;tree2\u0026quot;, # type of layout rotation = 2, # rotation of the layout nodeLabels = c(\u0026quot;var1\u0026quot;, \u0026quot;var2\u0026quot;, \u0026quot;var3\u0026quot;, \u0026quot;var4\u0026quot;, \u0026quot;var5\u0026quot;, \u0026quot;var6\u0026quot;, \u0026quot;var7\u0026quot;, \u0026quot;var8\u0026quot;, \u0026quot;var9\u0026quot;, \u0026quot;var10\u0026quot;, \u0026quot;Factor1\u0026quot;, \u0026quot;Factor2\u0026quot;))  In addition, the following visualization is often used when it comes to using the function semPaths(). There is a weighting of the edges depending on the standardized parameter estimates. semPaths(fit, # fitted model nCharNodes = 0, # no abbreviation in node labels what = \u0026quot;std\u0026quot;, # standardized parameter estimates as weighted edges whatLabels = \u0026quot;std\u0026quot;, # standardized parameter estimate residuals = F, # excluding residuals and variances sizeLat = 10, # width latent sizeLat2 = 10, # height latent sizeMan = 6, # width manifest edge.label.cex = 0.90, # font size of parameters layout = \u0026quot;tree2\u0026quot;, # type of layout rotation = 2) # rotation of the layout  Sampling Weights With lavaan version 0.6-3 there are two ways for weighting. On the one hand, you can use the package lavaan.surveyfor taking sampling weights into account. On the other hand, it’s possible to specify the sampling weights within the cfa() function. fit_2 \u0026lt;- cfa(model, data = data, estimator = \u0026quot;MLM\u0026quot;) library(lavaan.survey) # specify survey design sd_fit \u0026lt;- survey::svydesign(id = ~1, weights = ~weight, data = data) # fitting the model again while taking the survey design into account surveyfit \u0026lt;- lavaan.survey(lavaan.fit = fit_2, survey.design = sd_fit) summary(surveyfit, standardized = T) ## lavaan 0.6-3 ended normally after 38 iterations ## ## Optimization method NLMINB ## Number of free parameters 31 ## ## Number of observations 1914 ## ## Estimator ML Robust ## Model Fit Test Statistic 219.683 189.418 ## Degrees of freedom 34 34 ## P-value (Chi-square) 0.000 0.000 ## Scaling correction factor 1.160 ## for the Satorra-Bentler correction ## ## Parameter Estimates: ## ## Information Expected ## Information saturated (h1) model Structured ## Standard Errors Robust.sem ## ## Latent Variables: ## Estimate Std.Err z-value P(\u0026gt;|z|) Std.lv Std.all ## F1 =~ ## X1 1.000 1.182 0.655 ## X2 0.869 0.043 20.161 0.000 1.027 0.554 ## X3 0.863 0.042 20.567 0.000 1.020 0.679 ## X4 1.044 0.046 22.914 0.000 1.234 0.647 ## X5 0.882 0.044 20.199 0.000 1.042 0.625 ## X6 0.914 0.047 19.551 0.000 1.080 0.579 ## F2 =~ ## Y1 1.000 1.409 0.799 ## Y2 6.841 0.261 26.160 0.000 9.636 0.763 ## Y3 1.477 0.069 21.527 0.000 2.080 0.556 ## Y4 0.280 0.013 22.094 0.000 0.394 0.599 ## ## Regressions: ## Estimate Std.Err z-value P(\u0026gt;|z|) Std.lv Std.all ## F1 ~ ## F2 -0.207 0.026 -8.079 0.000 -0.247 -0.247 ## ## Intercepts: ## Estimate Std.Err z-value P(\u0026gt;|z|) Std.lv Std.all ## .X1 3.567 0.043 83.354 0.000 3.567 1.977 ## .X2 3.948 0.044 88.923 0.000 3.948 2.131 ## .X3 2.508 0.036 69.819 0.000 2.508 1.669 ## .X4 3.387 0.046 74.218 0.000 3.387 1.776 ## .X5 2.785 0.039 70.605 0.000 2.785 1.671 ## .X6 3.205 0.045 71.777 0.000 3.205 1.717 ## .Y1 4.435 0.042 104.668 0.000 4.435 2.515 ## .Y2 45.500 0.303 149.930 0.000 45.500 3.604 ## .Y3 14.259 0.090 158.422 0.000 14.259 3.814 ## .Y4 2.888 0.016 184.040 0.000 2.888 4.390 ## .F1 0.000 0.000 0.000 ## F2 0.000 0.000 0.000 ## ## Variances: ## Estimate Std.Err z-value P(\u0026gt;|z|) Std.lv Std.all ## .X1 1.857 0.083 22.438 0.000 1.857 0.571 ## .X2 2.377 0.084 28.178 0.000 2.377 0.693 ## .X3 1.217 0.062 19.478 0.000 1.217 0.539 ## .X4 2.115 0.089 23.802 0.000 2.115 0.581 ## .X5 1.691 0.073 23.149 0.000 1.691 0.609 ## .X6 2.317 0.094 24.653 0.000 2.317 0.665 ## .Y1 1.126 0.078 14.497 0.000 1.126 0.362 ## .Y2 66.544 3.832 17.366 0.000 66.544 0.418 ## .Y3 9.653 0.382 25.275 0.000 9.653 0.690 ## .Y4 0.278 0.011 25.751 0.000 0.278 0.641 ## .F1 1.312 0.090 14.643 0.000 0.939 0.939 ## F2 1.984 0.100 19.911 0.000 1.000 1.000 # weighting within the cfa() function weighted_fit \u0026lt;- cfa(model, data = data, estimator = \u0026quot;MLM\u0026quot;, sampling.weights = \u0026quot;weight\u0026quot;) # sepcify sampling weights summary(weighted_fit, standardized = T) ## lavaan 0.6-3 ended normally after 38 iterations ## ## Optimization method NLMINB ## Number of free parameters 21 ## ## Number of observations 1914 ## Sampling weights variable weight ## ## Estimator ML Robust ## Model Fit Test Statistic 219.683 191.128 ## Degrees of freedom 34 34 ## P-value (Chi-square) 0.000 0.000 ## Scaling correction factor 1.149 ## for the Yuan-Bentler correction (Mplus variant) ## ## Parameter Estimates: ## ## Information Observed ## Observed information based on Hessian ## Standard Errors Robust.huber.white ## ## Latent Variables: ## Estimate Std.Err z-value P(\u0026gt;|z|) Std.lv Std.all ## F1 =~ ## X1 1.000 1.182 0.655 ## X2 0.869 0.042 20.621 0.000 1.027 0.554 ## X3 0.863 0.043 19.851 0.000 1.020 0.679 ## X4 1.044 0.047 22.353 0.000 1.234 0.647 ## X5 0.882 0.046 18.983 0.000 1.042 0.625 ## X6 0.914 0.047 19.579 0.000 1.080 0.579 ## F2 =~ ## Y1 1.000 1.409 0.799 ## Y2 6.841 0.236 29.036 0.000 9.636 0.763 ## Y3 1.477 0.072 20.498 0.000 2.080 0.556 ## Y4 0.280 0.013 21.018 0.000 0.394 0.599 ## ## Regressions: ## Estimate Std.Err z-value P(\u0026gt;|z|) Std.lv Std.all ## F1 ~ ## F2 -0.207 0.026 -8.011 0.000 -0.247 -0.247 ## ## Variances: ## Estimate Std.Err z-value P(\u0026gt;|z|) Std.lv Std.all ## .X1 1.857 0.084 22.175 0.000 1.857 0.571 ## .X2 2.377 0.086 27.532 0.000 2.377 0.693 ## .X3 1.217 0.063 19.245 0.000 1.217 0.539 ## .X4 2.115 0.091 23.330 0.000 2.115 0.581 ## .X5 1.691 0.074 22.828 0.000 1.691 0.609 ## .X6 2.317 0.096 24.170 0.000 2.317 0.665 ## .Y1 1.126 0.075 15.077 0.000 1.126 0.362 ## .Y2 66.544 3.654 18.210 0.000 66.544 0.418 ## .Y3 9.653 0.389 24.803 0.000 9.653 0.690 ## .Y4 0.278 0.011 25.036 0.000 0.278 0.641 ## .F1 1.312 0.092 14.338 0.000 0.939 0.939 ## F2 1.984 0.097 20.377 0.000 1.000 1.000  ","date":1540339200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540339200,"objectID":"83d2481e7fbc72daf9ff06ab4b8b059d","permalink":"/post/structural-equation-modeling-in-r/","publishdate":"2018-10-24T00:00:00Z","relpermalink":"/post/structural-equation-modeling-in-r/","section":"post","summary":"lavaan: A Package for SEM Syntax for Modelling Model Fitting (Standardized) Estimated Parameter Change Visualization: Plotting SEM Sampling Weights   lavaan: A Package for SEM In my pervious post I made a short introduction to Structural Equation Modeling (SEM). But now it’s about using R for SEM. However, to use R for SEM we need the package lavaan and I will introduce the basic functions of the package. For the visualization of the models we also need the package semPlot.","tags":["SEM","Structural Equation Modeling","lavaan","semplot","fit indices"],"title":"Structural Equation Modeling in R","type":"post"},{"authors":null,"categories":["R","Tutorial"],"content":"Introduction \u0026amp; Advantages There are a lot of variations regarding Structural Equation Modeling (SEM). Therefore, this article is focusing on the basics of SEM. Fundamentally, SEM can be classified as a combination of path analysis and confirmatory factor analysis. With SEM and path analysis you will have a big felxibility for specifying relationships between theoretical constructs. For example, it is possible to postulate rather complex models that may include a single construct that is theorized to be a predictor of some constructs and is also predicted by other constructs. You can estimate the multiple and interrelated dependence in a single analysis. Moreover, there are two types of variables which are used in SEM. Variables which are used only as a predictor are classified ad exogenous, whereas variables that are predicted by at least one other variable are classified as endogenous. But what are the exact benefits of SEM? Regarding this question, two main advantages of SEM will be discussed below. Latent Variables SEM distinguished between measurement and latent variables. One major difference between SEM and other methods is the use of latent variables which are captured by measurement variables. Latent variables represent constructs that can not be observed. For example, constructs such as stability, well-being, statisfaction or xenophobia are too complex to be measured directly. Moreover, with SEM the measruement error which cannot be explained by the latent variable will be considered. So, in a measurement model you will have a measurement error for each measurement variable. If \\(F\\) is the latent variable and \\(X1\\) and \\(X2\\) are the measurement variables and there is no measurement error, the path of \\(F \\rightarrow X1\\) and \\(F \\rightarrow X2\\) would be \\(1.0\\). But as we now, that will be not the case. So, assume a path of \\(.7\\) for \\(F \\rightarrow X1\\) and \\(.8\\) for \\(F \\rightarrow X2\\). Consequently, there would be a correlation of \\(.56\\) between the two variables \\(X1\\) and \\(X2\\). The measurement error will take into account the difference between \\(1.0\\) and \\(.56\\). Because of this, the relationships between the latent constructs can be more accurate.  Simultaneous Estimation Regarding most methods, only the relationship of a dependent variable to one or more independent variables can be estimated. However, using SEM, it is also possible to estimate the relationship between multiple dependent variables. But the simultaneous estimation gives another advantage. Thus, in addition to the direct effect, the indirect and total effects can be easily calculated. Just a small example: \\(\\small Ind. \\ Effect_{X1\\rightarrow Y} = Dir. \\ Effect_{X1\\rightarrow X2}*Dir. \\ Effect_{X2\\rightarrow Y} = .9*-.47 = -.43\\) \\(\\small Tot. \\ Effect_{X1\\rightarrow Y} = Ind. \\ Effect_{X1\\rightarrow Y} + Dir. \\ Effect_{X1\\rightarrow Y} = -.43 -.42 = -.85\\)\n  Identification Generally, a structural equation model must be identified, otherwise a clear parameter estimation can not be made. The identification should be determined for all model levels:  Identification of the individual measurement models Identification of the structural model Identification of the overall model  The aim should be that there is over-identification at all model levels. In order to determine the degree of identification of a model, the following formula can be used (counting rule): \\[t \\leq\\frac{p(p+1)}{2}\\\\ p = observed \\ variables \\\\ t = estimated\\ paramters\\]\nThe same formula can be used to calculate the degree of freedoms as well:\n\\[df =\\frac{p(p+1)}{2} - t\\]\nConsequently, there are three scenarios for identification:\n \\(df \u0026lt; 0\\): The model is underidentified \\(df = 0\\): The model is exactly identified \\(df \u0026gt; 0\\): The model is overidentified  As mentioned above, the goal should be an overidentified model. This is needed to perform further tests (for example: testing the fit of the model).  Fit Indices At the beginning of SEM, testing the following null hypothesis was considered a good way to assess the model specification: \\[\\sum = \\sum(0)\\]\nWhere \\(\\sum\\) stands for the population covariance matrix and \\(\\sum{0}\\) for the covariance matrix implied by the specified model. To check this null hypothesis, a \\(\\chi^2\\)-test can be used. However, this test is extremely sensitive to high sample sizes1. For this reason, other methods have been developed to test the qualtiy of a model. In the following, some important Fit Indices will be discussed. \\(\\chi^2\\)-Test Despite the high sensitivity with high sample sizes, I would nevertheless explain the test briefly. The \\(\\chi^2\\) is calculated as follows, where O represents the observations and E represents the expected values: \\[\\chi^{2} = \\sum_{i=1}^{n}\\frac{(O_{i}-E_{i})^2}{E_{i}}\\]\nSince the null hypothesis should not be rejected, the test should not be significant. If it comes to a significant result, then the model would have to be rejected or optimized. But as already pointed out, with larger sample size, there will always be a significant result. Based on this test, most models would have to be rejected. For that reason you should not rely on this test.  CFI - Comparative Fit Index In contrast to the \\(\\chi^2\\)-test, the CFI takes into account the sample size and is reliable even for small samples: \\[CFI = 1-\\bigg(\\frac{\\lambda_{k}}{\\lambda_{i}}\\bigg) = 1 - \\bigg(\\frac{max[(\\chi^2_{t}-df_{t}), 0]}{max[(\\chi^2_{t}-df_{t}),(\\chi^2_{n}-df_{n}),0]}\\bigg)\\]\n\\(\\chi^2_{t}\\) and \\(\\chi^2_{n}\\) are the \\(\\chi^2\\) statistics for the target and the baseline model. The degree of freedoms of the target and the null model are represented by \\(df_{t}\\) and \\(df_{n}\\). However, the CFI evaluates the extent to which the tested model is superior to a alternative model in reproducing the observed covariance matrix. The range of the CFI is between \\(0\\) and \\(1\\), whereas a value of \\(1\\) indicates a perfect fit. The cut off value is \\(.95\\)2.  RMSEA - Root Mean Square Error of Approximation RMSEA expresses the discrepancy between the observed covariance matrix and the covariance matrix implied by the model per degree of freedom: \\[RMSEA = \\sqrt\\frac{\\hat{F_{0}}}{df_{t}}=\\sqrt\\frac{max\\Big[\\frac{(\\chi^2_{t}-df_{t})}{(N-1)},0\\Big]}{df_{t}}= \\sqrt\\frac{\\chi^2_{t} - df_{t}}{df_{t}(N-1)}\\]\nWhere \\(\\chi^2_{t}\\) is the \\(\\chi^2\\) statistic and \\(df_{t}\\) the degree of freedom for the target model. In contrast to the CFI, the RMSEA does not perform well with small sample sizes. So, it tends to reject a true model, if the sample size is small. However, the RMSEA does include the model complexity. As with the CFI, there is a range from \\(0\\) to \\(1\\), where a value of \\(0\\) implies a perfect fit. A value of \\(\\leq.05\\) implies a very good fit, a value between \\(.05\\) and \\(.08\\) is considered with a good fit and a value between \\(.08\\) and \\(.10\\) represents a acceptable fit. However, if the RMSEA is greater than \\(.10\\) the fit of the model is bad2.  SRMR - Standardized Root Mean Square Residual The SRMR is the root of the difference between the residuals of the covaruance matrix implied by the specified model and the3: \\[SRMR = \\sqrt{\\frac{\\Bigg\\{2\\sum_{i=1}^{p}\\sum_{j=1}^{i}\\Bigg[\\frac{(s_{ij}-\\hat\\sigma_{ij})}{(s_{ii}*s_{jj})}\\Bigg]^2\\Bigg\\}}{p(p+1)}}\\]\n\\(s_{ii}\\) and \\(s_{jj}\\) are the observed standard deviations. \\(\\sigma_{ij}\\) is the covariance matrix implied by the model, wheras \\(s_{ij}\\) is the observed covariance matrix. A value of \\(0\\) implies a perfect fit. The SRMR should not be greater than \\(.08\\)2.   Model Optimization The modification indices can be used to identify potential model misspecification. If the standardized estimated parameter change (SEPC) is \\(\\geq.20\\), there may be a possible misspecification4. If you want to optimize the model, the parameters can be freely estimated, in which the SEPC is \\(\\geq.20\\). However, this quickly results in parameters being freely estimated where theoretical justification is only partially possible. Thus, error correlations are often freely estimated, which are theoretically rarely well justified5. The fit of the model can indeed be easily optimized in this way. But what brings a better fit, if the resulting model is no longer similar to the theory to be tested? Right, such a model offers little added value. Therefore, a model should only be optimized if this is theoretically well justified.  References 1 Cangur, S. and Ercan, I. 2015: Comparison of model fit indices used in structural equation modeling under multivariate normality. Journal of Modern Applied Statistical Methods. 14 (1), 14.  2 Hu, L.-t. and Bentler, P.M. 1999: Cutoff criteria for fit indexes in covariance structure analysis: Conventional criteria versus new alternatives. Structural Equation Modeling: A Multidisciplinary Journal. 6 (1), 1–55.  3 Chen, F.F. 2007: Sensitivity of goodness of fit indexes to lack of measurement invariance. Structural Equation Modeling. 14 (3), 464–504.  4 Whittaker, T.A. 2012: Using the modification index and standardized expected parameter change for model modification. The Journal of Experimental Education. 80 (1), 26–44.  5 Hermida, R. 2015: The problem of allowing correlated errors in structural equation modeling: Concerns and considerations. Computational Methods in Social Sciences. 3 (1), 5.    ","date":1539388800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539388800,"objectID":"a046aa2becb3418b92874f13d1748b56","permalink":"/post/structural-equation-modeling-a-short-introduction/","publishdate":"2018-10-13T00:00:00Z","relpermalink":"/post/structural-equation-modeling-a-short-introduction/","section":"post","summary":"Introduction \u0026amp; Advantages There are a lot of variations regarding Structural Equation Modeling (SEM). Therefore, this article is focusing on the basics of SEM. Fundamentally, SEM can be classified as a combination of path analysis and confirmatory factor analysis. With SEM and path analysis you will have a big felxibility for specifying relationships between theoretical constructs. For example, it is possible to postulate rather complex models that may include a single construct that is theorized to be a predictor of some constructs and is also predicted by other constructs.","tags":["Structural Equation Modeling","SEM","fit indices","lavaan","semplot"],"title":"Structural Equation Modeling: A Short Introduction","type":"post"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;\n","date":1536444000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536444000,"objectID":"6a451186c775f5f0adb3a0416d0cb711","permalink":"/tutorial/example/","publishdate":"2018-09-09T00:00:00+02:00","relpermalink":"/tutorial/example/","section":"tutorial","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;","tags":null,"title":"Example Page","type":"docs"}]