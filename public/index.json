[{"authors":null,"categories":["R","Tutorial"],"content":"Introduction to dplyr When it comes to data manipulation, dplyr is a very powerful package with very intuitive functions. There are six important functions you will use often, if it comes to data manipulation:\n filter() select() mutate() arrange() group_by() summarise()  In addition to these functions dplyr offers you the ability to use the pipe-operator out of the magrittr package. This operator is very useful and I highly recommend you to get used to it. Except these functions, dplyr offers a lot more functions which are very useful. And, like always, you should look into the reference manual of dplyr for more details! I will mostly use the data frame gapminder for explaining the functions.\n filter() In many cases you need to filter your dataset because you are only interested in specific cases or you want to exclude some cases. For that, the filter() function provides an easy way to do that.\nLet’s take a look into the gapminder df, which is an excerpt of the Gapminder data on life expectancy, GDP per capita, and population by country:\nlibrary(dplyr) library(gapminder) knitr::kable(head(gapminder))   country continent year lifeExp pop gdpPercap    Afghanistan Asia 1952 28.801 8425333 779.4453  Afghanistan Asia 1957 30.332 9240934 820.8530  Afghanistan Asia 1962 31.997 10267083 853.1007  Afghanistan Asia 1967 34.020 11537966 836.1971  Afghanistan Asia 1972 36.088 13079460 739.9811  Afghanistan Asia 1977 38.438 14880372 786.1134    Now, different aspects of the data may be of interest:\n data for specific year(s) data for specific continent(s) data for specific continent(s) and year(s)  A very simple filtering could look like this one:\n# filtering by the year 1997 gapminder %\u0026gt;% filter(year == 1997) %\u0026gt;% head() %\u0026gt;% kableExtra::kable()    country  continent  year  lifeExp  pop  gdpPercap      Afghanistan  Asia  1997  41.763  22227415  635.3414    Albania  Europe  1997  72.950  3428038  3193.0546    Algeria  Africa  1997  69.152  29072015  4797.2951    Angola  Africa  1997  40.963  9875024  2277.1409    Argentina  Americas  1997  73.275  36203463  10967.2820    Australia  Oceania  1997  78.830  18565243  26997.9366     It is also possible to filter with more than one condition. If we are only interested in the year 1997 and the continent asia, we could use the following code.\n# filtering by the year 1997 and the continent asia gapminder %\u0026gt;% filter(year == 1997 \u0026amp; continent == \u0026quot;Asia\u0026quot;) %\u0026gt;% head() %\u0026gt;% kableExtra::kable()    country  continent  year  lifeExp  pop  gdpPercap      Afghanistan  Asia  1997  41.763  22227415  635.3414    Bahrain  Asia  1997  73.925  598561  20292.0168    Bangladesh  Asia  1997  59.412  123315288  972.7700    Cambodia  Asia  1997  56.534  11782962  734.2852    China  Asia  1997  70.426  1230075000  2289.2341    Hong Kong, China  Asia  1997  80.000  6495918  28377.6322     As you can imagine, there are many logical operators that you can use. I guess, most of you know these operators already.\n \u0026lt;: less than \u0026lt;=: less than or equal to ==: equal to !=: not equal to \u0026gt;=: greater than or eqal to \u0026gt;: greater than %in%: matching argument !x: not x (e.g. often used for NA exclusion !is.na(variable))  The following operators can be used to combine different conditions:\n \u0026amp;: filtering by condition 1 and contiditon 2 |: filtering by condition 1 or condition 2  You will see a few of these operators in action later.\n select() function The select() function is a very simple one but can be useful, if the data frame is very large but you only need a few variables. You can select specific coloumns and you can also delete sepcific coloumns.\n# let\u0026#39;s select the coloumns country, continent, year and lifeExp gapminder %\u0026gt;% select(country, continent, year, lifeExp) %\u0026gt;% head() %\u0026gt;% kableExtra::kable()    country  continent  year  lifeExp      Afghanistan  Asia  1952  28.801    Afghanistan  Asia  1957  30.332    Afghanistan  Asia  1962  31.997    Afghanistan  Asia  1967  34.020    Afghanistan  Asia  1972  36.088    Afghanistan  Asia  1977  38.438     # a shorter way to select these coloumns gapminder %\u0026gt;% select(country:lifeExp) %\u0026gt;% head() %\u0026gt;% kableExtra::kable()    country  continent  year  lifeExp      Afghanistan  Asia  1952  28.801    Afghanistan  Asia  1957  30.332    Afghanistan  Asia  1962  31.997    Afghanistan  Asia  1967  34.020    Afghanistan  Asia  1972  36.088    Afghanistan  Asia  1977  38.438     If we want to delete only a few columns of the df, we can do this by using -:\n# deleting the column population gapminder %\u0026gt;% select(-pop) %\u0026gt;% head() %\u0026gt;% kableExtra::kable()    country  continent  year  lifeExp  gdpPercap      Afghanistan  Asia  1952  28.801  779.4453    Afghanistan  Asia  1957  30.332  820.8530    Afghanistan  Asia  1962  31.997  853.1007    Afghanistan  Asia  1967  34.020  836.1971    Afghanistan  Asia  1972  36.088  739.9811    Afghanistan  Asia  1977  38.438  786.1134      mutate() function You can create new columns with the mutate() function. Moreover, it is possible to use the mutate() function in combination with other functions (e.g. case_when()). In the following code I create a new variable, where the value is 10% of the population size.\n# creating a new coloumn where the value is 10% of the population gapminder %\u0026gt;% mutate(new_pop = round(pop*0.1)) %\u0026gt;% head() %\u0026gt;% kableExtra::kable()    country  continent  year  lifeExp  pop  gdpPercap  new_pop      Afghanistan  Asia  1952  28.801  8425333  779.4453  842533    Afghanistan  Asia  1957  30.332  9240934  820.8530  924093    Afghanistan  Asia  1962  31.997  10267083  853.1007  1026708    Afghanistan  Asia  1967  34.020  11537966  836.1971  1153797    Afghanistan  Asia  1972  36.088  13079460  739.9811  1307946    Afghanistan  Asia  1977  38.438  14880372  786.1134  1488037     Now, let’s create a new variable with three categories. Therefore, we will use the mutate() function and the case_when() function in combination. In the following example the new variable should include three categories: low life expectancy (lifeExp \u0026lt; 50), medium life expectancy (lifeExp \u0026gt;= 50 \u0026amp; \u0026lt;70) and high life expectancy (\u0026gt;= 70).\n# creating three categories gapminder %\u0026gt;% mutate(lifeExp_cat = case_when( lifeExp \u0026lt; 50 ~ \u0026quot;low\u0026quot;, lifeExp \u0026lt; 70 \u0026amp; lifeExp \u0026gt;= 50 ~ \u0026quot;medium\u0026quot;, lifeExp \u0026gt;= 70 ~ \u0026quot;high\u0026quot; )) %\u0026gt;% head() %\u0026gt;% kableExtra::kable()    country  continent  year  lifeExp  pop  gdpPercap  lifeExp_cat      Afghanistan  Asia  1952  28.801  8425333  779.4453  low    Afghanistan  Asia  1957  30.332  9240934  820.8530  low    Afghanistan  Asia  1962  31.997  10267083  853.1007  low    Afghanistan  Asia  1967  34.020  11537966  836.1971  low    Afghanistan  Asia  1972  36.088  13079460  739.9811  low    Afghanistan  Asia  1977  38.438  14880372  786.1134  low     As you can see, you can add a bunch of conditions with the case_when() function. Moreover, with this combination of these functions it is very easy to manipulate specific cases if it’s needed. Personally, I use the combination of these functions very often. In this way, it is very easy to implement specific encodings.\n group_by() \u0026amp; summarise() function If you take a look into the data for the first time, it is often very useful to make some exploratory data analysis. The group_by() function in combination with the summarise() function makes it easy. For example, we can group by continent, calculate the mean for the life expectancy and plot the results for each continent over the time. I did that all with the pipe operator (%\u0026gt;%). If you are unfamiliar with the use of this operator, you should familiarize yourself with it. In my opinion, you can work much more efficiently with it.\nlibrary(ggplot2) gapminder %\u0026gt;% group_by(continent, year) %\u0026gt;% summarise(mean_lifeExp = mean(lifeExp)) %\u0026gt;% ggplot(aes(x = year, y = mean_lifeExp, col = continent)) + geom_line()  ","date":1541808000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541808000,"objectID":"9ba9ff53b539e626fea400e221010de2","permalink":"/post/data-wrangling-part-1-introduction-to-dplyr/","publishdate":"2018-11-10T00:00:00Z","relpermalink":"/post/data-wrangling-part-1-introduction-to-dplyr/","section":"post","summary":"Introduction to dplyr When it comes to data manipulation, dplyr is a very powerful package with very intuitive functions. There are six important functions you will use often, if it comes to data manipulation:\n filter() select() mutate() arrange() group_by() summarise()  In addition to these functions dplyr offers you the ability to use the pipe-operator out of the magrittr package. This operator is very useful and I highly recommend you to get used to it.","tags":["dplyr","data wrangling","data manipulation"],"title":"Data Wrangling Part 1: Introduction to dplyr","type":"post"},{"authors":null,"categories":["R","Tutorial"],"content":" lavaan: A Package for SEM Syntax for Modelling Model Fitting (Standardized) Estimated Parameter Change Visualization: Plotting SEM Sampling Weights   lavaan: A Package for SEM In my pervious post I made a short introduction to Structural Equation Modeling (SEM). But now it’s about using R for SEM. However, to use R for SEM we need the package lavaan and I will introduce the basic functions of the package. For the visualization of the models we also need the package semPlot. As you probably know already the installation of the packages is very simple: install.packages(\u0026quot;lavaan\u0026quot;) install.packages(\u0026quot;semPlot\u0026quot;)  Syntax for Modelling The modely syntax of lavaan is used for specifying a SEM and is also very intuitive. There are three operators for model specification:  =~ is used to specify latent variables/factors ~~ is used to specify (residual) (co)variances ~ is used to specify rgeressions  An example model specification might look like this:\nexample \u0026lt;- \u0026quot; # measurement model F1 =~ 1*V1 + V2 + V3 F2 =~ 1*V4 + V5 + V6 # (residual) (co)variances V1 ~~ V1 # variance V1 ~~ V3 # residual correlation # regressions F1 ~ F2\u0026quot;  Model Fitting Now, let’s specify the first model. I will use an own dataset from a previous work. It’s also possible to use the built-in datasets HolzingerSwineford1939 or PolitcialDemocracy from lavaan to become familiar with the package. Moreover, the functions cfa() and sem() are currently similar but they may differ in the future. It’s also possible to use one of the robust estimators, if you’re fitting the model. library(lavaan) model \u0026lt;- \u0026quot; # measurement model F1 =~ 1*X1 + X2 + X3 + X4 + X5 + X6 F2 =~ 1*Y1 + Y2 + Y3 + Y4 # regression F1 ~ F2\u0026quot; # fitting the model fit \u0026lt;- cfa(model, data = data, estimator = \u0026quot;MLM\u0026quot;) # robust estimator # summary of the fit with standardized values and fit measurements summary(fit, standardized = T, fit.measures = T) ## lavaan 0.6-3 ended normally after 39 iterations ## ## Optimization method NLMINB ## Number of free parameters 21 ## ## Number of observations 1914 ## ## Estimator ML Robust ## Model Fit Test Statistic 218.644 204.758 ## Degrees of freedom 34 34 ## P-value (Chi-square) 0.000 0.000 ## Scaling correction factor 1.068 ## for the Satorra-Bentler correction ## ## Model test baseline model: ## ## Minimum Function Test Statistic 4983.424 4648.582 ## Degrees of freedom 45 45 ## P-value 0.000 0.000 ## ## User model versus baseline model: ## ## Comparative Fit Index (CFI) 0.963 0.963 ## Tucker-Lewis Index (TLI) 0.951 0.951 ## ## Robust Comparative Fit Index (CFI) 0.963 ## Robust Tucker-Lewis Index (TLI) 0.951 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -39038.962 -39038.962 ## Loglikelihood unrestricted model (H1) -38929.641 -38929.641 ## ## Number of free parameters 21 21 ## Akaike (AIC) 78119.925 78119.925 ## Bayesian (BIC) 78236.621 78236.621 ## Sample-size adjusted Bayesian (BIC) 78169.903 78169.903 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.053 0.051 ## 90 Percent Confidence Interval 0.047 0.060 0.045 0.058 ## P-value RMSEA \u0026lt;= 0.05 0.203 0.366 ## ## Robust RMSEA 0.053 ## 90 Percent Confidence Interval 0.046 0.060 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.036 0.036 ## ## Parameter Estimates: ## ## Information Expected ## Information saturated (h1) model Structured ## Standard Errors Robust.sem ## ## Latent Variables: ## Estimate Std.Err z-value P(\u0026gt;|z|) Std.lv Std.all ## F1 =~ ## X1 1.000 1.197 0.652 ## X2 0.839 0.042 20.082 0.000 1.004 0.541 ## X3 0.857 0.040 21.283 0.000 1.026 0.679 ## X4 1.035 0.044 23.642 0.000 1.239 0.646 ## X5 0.884 0.042 20.917 0.000 1.058 0.621 ## X6 0.916 0.045 20.517 0.000 1.096 0.584 ## F2 =~ ## Y1 1.000 1.413 0.803 ## Y2 6.793 0.243 28.008 0.000 9.597 0.763 ## Y3 1.456 0.065 22.322 0.000 2.057 0.554 ## Y4 0.280 0.012 23.170 0.000 0.395 0.596 ## ## Regressions: ## Estimate Std.Err z-value P(\u0026gt;|z|) Std.lv Std.all ## F1 ~ ## F2 -0.228 0.024 -9.345 0.000 -0.270 -0.270 ## ## Variances: ## Estimate Std.Err z-value P(\u0026gt;|z|) Std.lv Std.all ## .X1 1.933 0.083 23.316 0.000 1.933 0.574 ## .X2 2.440 0.082 29.879 0.000 2.440 0.708 ## .X3 1.232 0.059 20.803 0.000 1.232 0.539 ## .X4 2.140 0.086 25.025 0.000 2.140 0.582 ## .X5 1.783 0.076 23.567 0.000 1.783 0.614 ## .X6 2.318 0.088 26.265 0.000 2.318 0.659 ## .Y1 1.101 0.072 15.382 0.000 1.101 0.355 ## .Y2 66.303 3.638 18.223 0.000 66.303 0.419 ## .Y3 9.577 0.369 25.929 0.000 9.577 0.694 ## .Y4 0.284 0.011 26.420 0.000 0.284 0.645 ## .F1 1.328 0.088 15.050 0.000 0.927 0.927 ## F2 1.996 0.093 21.441 0.000 1.000 1.000 If you want to select specific fit measurements, you can use the function fitmeasures(). By using the command fit.measures = \u0026quot;all\u0026quot; you will get all available fit measures. fitmeasures(fit, fit.measures = c(\u0026quot;chisq\u0026quot;, \u0026quot;cfi\u0026quot;, \u0026quot;rmsea\u0026quot;, \u0026quot;srmr\u0026quot;)) %\u0026gt;% kableExtra::kable()     x      chisq  218.64363505    cfi  0.96261082    rmsea  0.05326682    srmr  0.03600232     Apart from the fit indices, the standardized residuals can also be used to interpret the model fit. Unfortunately there is no function to get the standardized residuals. However, a function can be written very fast for this. Using cov2cor on the covariance matrices gives us the fitted and observed correlation matrices. To get the residual correlation matrix we need to subtracting the fitted correlation matrix from the observed correlation matrix. std_residuals \u0026lt;- function(fit) { cov \u0026lt;- list(observed = inspect(fit, \u0026quot;sampstat\u0026quot;)$cov, fitted = fitted(fit)$cov) cor \u0026lt;- list(observed = cov2cor(cov$observed), fitted = cov2cor(cov$fitted)) cor$residual \u0026lt;- cor$observed - cor$fitted lapply(cor, function(x) round(x, 2)) } std_residuals(fit) ## $observed ## X1 X2 X3 X4 X5 X6 Y1 Y2 Y3 Y4 ## X1 1.00 ## X2 0.39 1.00 ## X3 0.43 0.44 1.00 ## X4 0.42 0.30 0.43 1.00 ## X5 0.38 0.30 0.42 0.42 1.00 ## X6 0.41 0.28 0.35 0.43 0.38 1.00 ## Y1 -0.13 -0.10 -0.18 -0.13 -0.18 -0.06 1.00 ## Y2 -0.13 -0.09 -0.16 -0.13 -0.17 -0.08 0.63 1.00 ## Y3 -0.08 -0.08 -0.13 -0.05 -0.13 0.01 0.43 0.39 1.00 ## Y4 -0.12 -0.07 -0.13 -0.10 -0.18 -0.05 0.46 0.43 0.43 1.00 ## ## $fitted ## X1 X2 X3 X4 X5 X6 Y1 Y2 Y3 Y4 ## X1 1.00 ## X2 0.35 1.00 ## X3 0.44 0.37 1.00 ## X4 0.42 0.35 0.44 1.00 ## X5 0.41 0.34 0.42 0.40 1.00 ## X6 0.38 0.32 0.40 0.38 0.36 1.00 ## Y1 -0.14 -0.12 -0.15 -0.14 -0.13 -0.13 1.00 ## Y2 -0.13 -0.11 -0.14 -0.13 -0.13 -0.12 0.61 1.00 ## Y3 -0.10 -0.08 -0.10 -0.10 -0.09 -0.09 0.44 0.42 1.00 ## Y4 -0.10 -0.09 -0.11 -0.10 -0.10 -0.09 0.48 0.45 0.33 1.00 ## ## $residual ## X1 X2 X3 X4 X5 X6 Y1 Y2 Y3 Y4 ## X1 0.00 ## X2 0.04 0.00 ## X3 -0.01 0.07 0.00 ## X4 -0.01 -0.05 -0.01 0.00 ## X5 -0.03 -0.04 0.00 0.02 0.00 ## X6 0.03 -0.04 -0.05 0.05 0.02 0.00 ## Y1 0.01 0.02 -0.03 0.01 -0.05 0.07 0.00 ## Y2 0.01 0.03 -0.02 0.00 -0.04 0.04 0.02 0.00 ## Y3 0.01 0.00 -0.02 0.05 -0.04 0.10 -0.02 -0.03 0.00 ## Y4 -0.01 0.01 -0.03 0.00 -0.08 0.05 -0.02 -0.02 0.10 0.00  (Standardized) Estimated Parameter Change To identify potential model misspecification, you can use modificationindices(). If the SEPC is \\(\\geq.20\\), there may be a possible misspecification. However, adjustments to the model should be based on a theoretical justification. # first 10 rows as output \u0026amp; sorting by mi-value head(modificationindices(fit, sort. = T), 10) %\u0026gt;% dplyr::select(-sepc.lv, -sepc.nox) %\u0026gt;% kableExtra::kable()     lhs  op  rhs  mi  epc  sepc.all      78  Y3  ~~  Y4  59.82491  0.3450967  0.2091268    73  Y1  ~~  Y2  50.65517  4.9004680  0.5736744    43  X2  ~~  X3  44.57299  0.3354141  0.1934529    33  F2  =~  X6  21.73991  0.1452641  0.1094232    53  X3  ~~  X6  20.47615  -0.2289873  -0.1355164    59  X4  ~~  X6  20.35185  0.2900851  0.1302539    44  X2  ~~  X4  20.23462  -0.2881275  -0.1260850    32  F2  =~  X5  16.26564  -0.1121514  -0.0930021    75  Y1  ~~  Y4  14.99285  -0.0978666  -0.1749505    71  X6  ~~  Y3  13.63301  0.4394287  0.0932713      Visualization: Plotting SEM For visualization of an structural equation model the package semPlot offers many possibilities. I would recommend you to look in the reference manual of this package. There are a lot of specifications you can do. However, the specification that I’m using should cover everything necessary for the beginning. In general, an adjustment of the rotation and the layout will probably be necessary. library(semPlot) semPaths(fit, # fitted model nCharNodes = 0, # no abbreviation in node labels whatLabels = \u0026quot;std\u0026quot;, # standardized parameter estimate residuals = F, # excluding residuals and variances sizeLat = 10, # width latent sizeLat2 = 10, # height latent sizeMan = 6, # width manifest edge.label.cex = 0.90, # font size of parameters layout = \u0026quot;tree2\u0026quot;, # type of layout rotation = 2) # rotation of the layout If you want to edit the label of the nodes, you can do this by using the nodeLabels argument. To do so, the order of the nodes must first be determined.\n# determine the order of the nodes semPaths(fit, # fitted model nCharNodes = 0, # no abbreviation in node labels whatLabels = \u0026quot;std\u0026quot;, # standardized parameter estimate residuals = F, # excluding residuals and variances sizeLat = 10, # width latent sizeLat2 = 10, # height latent sizeMan = 6, # width manifest edge.label.cex = 0.90, # font size of parameters layout = \u0026quot;tree2\u0026quot;, # type of layout rotation = 2, # rotation of the layout nodeLabels = letters[1:12]) # getting the order of the nodes # labeling the nodes semPaths(fit, # fitted model nCharNodes = 0, # no abbreviation in node labels whatLabels = \u0026quot;std\u0026quot;, # standardized parameter estimate residuals = F, # excluding residuals and variances sizeLat = 10, # width latent sizeLat2 = 10, # height latent sizeMan = 6, # width manifest edge.label.cex = 0.90, # font size of parameters layout = \u0026quot;tree2\u0026quot;, # type of layout rotation = 2, # rotation of the layout nodeLabels = c(\u0026quot;var1\u0026quot;, \u0026quot;var2\u0026quot;, \u0026quot;var3\u0026quot;, \u0026quot;var4\u0026quot;, \u0026quot;var5\u0026quot;, \u0026quot;var6\u0026quot;, \u0026quot;var7\u0026quot;, \u0026quot;var8\u0026quot;, \u0026quot;var9\u0026quot;, \u0026quot;var10\u0026quot;, \u0026quot;Factor1\u0026quot;, \u0026quot;Factor2\u0026quot;))  In addition, the following visualization is often used when it comes to using the function semPaths(). There is a weighting of the edges depending on the standardized parameter estimates. semPaths(fit, # fitted model nCharNodes = 0, # no abbreviation in node labels what = \u0026quot;std\u0026quot;, # standardized parameter estimates as weighted edges whatLabels = \u0026quot;std\u0026quot;, # standardized parameter estimate residuals = F, # excluding residuals and variances sizeLat = 10, # width latent sizeLat2 = 10, # height latent sizeMan = 6, # width manifest edge.label.cex = 0.90, # font size of parameters layout = \u0026quot;tree2\u0026quot;, # type of layout rotation = 2) # rotation of the layout  Sampling Weights With lavaan version 0.6-3 there are two ways for weighting. On the one hand, you can use the package lavaan.surveyfor taking sampling weights into account. On the other hand, it’s possible to specify the sampling weights within the cfa() function. fit_2 \u0026lt;- cfa(model, data = data, estimator = \u0026quot;MLM\u0026quot;) library(lavaan.survey) # specify survey design sd_fit \u0026lt;- survey::svydesign(id = ~1, weights = ~weight, data = data) # fitting the model again while taking the survey design into account surveyfit \u0026lt;- lavaan.survey(lavaan.fit = fit_2, survey.design = sd_fit) summary(surveyfit, standardized = T) ## lavaan 0.6-3 ended normally after 38 iterations ## ## Optimization method NLMINB ## Number of free parameters 31 ## ## Number of observations 1914 ## ## Estimator ML Robust ## Model Fit Test Statistic 219.683 189.418 ## Degrees of freedom 34 34 ## P-value (Chi-square) 0.000 0.000 ## Scaling correction factor 1.160 ## for the Satorra-Bentler correction ## ## Parameter Estimates: ## ## Information Expected ## Information saturated (h1) model Structured ## Standard Errors Robust.sem ## ## Latent Variables: ## Estimate Std.Err z-value P(\u0026gt;|z|) Std.lv Std.all ## F1 =~ ## X1 1.000 1.182 0.655 ## X2 0.869 0.043 20.161 0.000 1.027 0.554 ## X3 0.863 0.042 20.567 0.000 1.020 0.679 ## X4 1.044 0.046 22.914 0.000 1.234 0.647 ## X5 0.882 0.044 20.199 0.000 1.042 0.625 ## X6 0.914 0.047 19.551 0.000 1.080 0.579 ## F2 =~ ## Y1 1.000 1.409 0.799 ## Y2 6.841 0.261 26.160 0.000 9.636 0.763 ## Y3 1.477 0.069 21.527 0.000 2.080 0.556 ## Y4 0.280 0.013 22.094 0.000 0.394 0.599 ## ## Regressions: ## Estimate Std.Err z-value P(\u0026gt;|z|) Std.lv Std.all ## F1 ~ ## F2 -0.207 0.026 -8.079 0.000 -0.247 -0.247 ## ## Intercepts: ## Estimate Std.Err z-value P(\u0026gt;|z|) Std.lv Std.all ## .X1 3.567 0.043 83.354 0.000 3.567 1.977 ## .X2 3.948 0.044 88.923 0.000 3.948 2.131 ## .X3 2.508 0.036 69.819 0.000 2.508 1.669 ## .X4 3.387 0.046 74.218 0.000 3.387 1.776 ## .X5 2.785 0.039 70.605 0.000 2.785 1.671 ## .X6 3.205 0.045 71.777 0.000 3.205 1.717 ## .Y1 4.435 0.042 104.668 0.000 4.435 2.515 ## .Y2 45.500 0.303 149.930 0.000 45.500 3.604 ## .Y3 14.259 0.090 158.422 0.000 14.259 3.814 ## .Y4 2.888 0.016 184.040 0.000 2.888 4.390 ## .F1 0.000 0.000 0.000 ## F2 0.000 0.000 0.000 ## ## Variances: ## Estimate Std.Err z-value P(\u0026gt;|z|) Std.lv Std.all ## .X1 1.857 0.083 22.438 0.000 1.857 0.571 ## .X2 2.377 0.084 28.178 0.000 2.377 0.693 ## .X3 1.217 0.062 19.478 0.000 1.217 0.539 ## .X4 2.115 0.089 23.802 0.000 2.115 0.581 ## .X5 1.691 0.073 23.149 0.000 1.691 0.609 ## .X6 2.317 0.094 24.653 0.000 2.317 0.665 ## .Y1 1.126 0.078 14.497 0.000 1.126 0.362 ## .Y2 66.544 3.832 17.366 0.000 66.544 0.418 ## .Y3 9.653 0.382 25.275 0.000 9.653 0.690 ## .Y4 0.278 0.011 25.751 0.000 0.278 0.641 ## .F1 1.312 0.090 14.643 0.000 0.939 0.939 ## F2 1.984 0.100 19.911 0.000 1.000 1.000 # weighting within the cfa() function weighted_fit \u0026lt;- cfa(model, data = data, estimator = \u0026quot;MLM\u0026quot;, sampling.weights = \u0026quot;weight\u0026quot;) # sepcify sampling weights summary(weighted_fit, standardized = T) ## lavaan 0.6-3 ended normally after 38 iterations ## ## Optimization method NLMINB ## Number of free parameters 21 ## ## Number of observations 1914 ## Sampling weights variable weight ## ## Estimator ML Robust ## Model Fit Test Statistic 219.683 191.128 ## Degrees of freedom 34 34 ## P-value (Chi-square) 0.000 0.000 ## Scaling correction factor 1.149 ## for the Yuan-Bentler correction (Mplus variant) ## ## Parameter Estimates: ## ## Information Observed ## Observed information based on Hessian ## Standard Errors Robust.huber.white ## ## Latent Variables: ## Estimate Std.Err z-value P(\u0026gt;|z|) Std.lv Std.all ## F1 =~ ## X1 1.000 1.182 0.655 ## X2 0.869 0.042 20.621 0.000 1.027 0.554 ## X3 0.863 0.043 19.851 0.000 1.020 0.679 ## X4 1.044 0.047 22.353 0.000 1.234 0.647 ## X5 0.882 0.046 18.983 0.000 1.042 0.625 ## X6 0.914 0.047 19.579 0.000 1.080 0.579 ## F2 =~ ## Y1 1.000 1.409 0.799 ## Y2 6.841 0.236 29.036 0.000 9.636 0.763 ## Y3 1.477 0.072 20.498 0.000 2.080 0.556 ## Y4 0.280 0.013 21.018 0.000 0.394 0.599 ## ## Regressions: ## Estimate Std.Err z-value P(\u0026gt;|z|) Std.lv Std.all ## F1 ~ ## F2 -0.207 0.026 -8.011 0.000 -0.247 -0.247 ## ## Variances: ## Estimate Std.Err z-value P(\u0026gt;|z|) Std.lv Std.all ## .X1 1.857 0.084 22.175 0.000 1.857 0.571 ## .X2 2.377 0.086 27.532 0.000 2.377 0.693 ## .X3 1.217 0.063 19.245 0.000 1.217 0.539 ## .X4 2.115 0.091 23.330 0.000 2.115 0.581 ## .X5 1.691 0.074 22.828 0.000 1.691 0.609 ## .X6 2.317 0.096 24.170 0.000 2.317 0.665 ## .Y1 1.126 0.075 15.077 0.000 1.126 0.362 ## .Y2 66.544 3.654 18.210 0.000 66.544 0.418 ## .Y3 9.653 0.389 24.803 0.000 9.653 0.690 ## .Y4 0.278 0.011 25.036 0.000 0.278 0.641 ## .F1 1.312 0.092 14.338 0.000 0.939 0.939 ## F2 1.984 0.097 20.377 0.000 1.000 1.000  ","date":1540339200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540339200,"objectID":"83d2481e7fbc72daf9ff06ab4b8b059d","permalink":"/post/structural-equation-modeling-in-r/","publishdate":"2018-10-24T00:00:00Z","relpermalink":"/post/structural-equation-modeling-in-r/","section":"post","summary":"lavaan: A Package for SEM Syntax for Modelling Model Fitting (Standardized) Estimated Parameter Change Visualization: Plotting SEM Sampling Weights   lavaan: A Package for SEM In my pervious post I made a short introduction to Structural Equation Modeling (SEM). But now it’s about using R for SEM. However, to use R for SEM we need the package lavaan and I will introduce the basic functions of the package. For the visualization of the models we also need the package semPlot.","tags":["SEM","Structural Equation Modeling","lavaan","semplot","fit indices"],"title":"Structural Equation Modeling in R","type":"post"},{"authors":null,"categories":["R","Tutorial"],"content":"Introduction \u0026amp; Advantages There are a lot of variations regarding Structural Equation Modeling (SEM). Therefore, this article is focusing on the basics of SEM. Fundamentally, SEM can be classified as a combination of path analysis and confirmatory factor analysis. With SEM and path analysis you will have a big felxibility for specifying relationships between theoretical constructs. For example, it is possible to postulate rather complex models that may include a single construct that is theorized to be a predictor of some constructs and is also predicted by other constructs. You can estimate the multiple and interrelated dependence in a single analysis. Moreover, there are two types of variables which are used in SEM. Variables which are used only as a predictor are classified ad exogenous, whereas variables that are predicted by at least one other variable are classified as endogenous. But what are the exact benefits of SEM? Regarding this question, two main advantages of SEM will be discussed below. Latent Variables SEM distinguished between measurement and latent variables. One major difference between SEM and other methods is the use of latent variables which are captured by measurement variables. Latent variables represent constructs that can not be observed. For example, constructs such as stability, well-being, statisfaction or xenophobia are too complex to be measured directly. Moreover, with SEM the measruement error which cannot be explained by the latent variable will be considered. So, in a measurement model you will have a measurement error for each measurement variable. If \\(F\\) is the latent variable and \\(X1\\) and \\(X2\\) are the measurement variables and there is no measurement error, the path of \\(F \\rightarrow X1\\) and \\(F \\rightarrow X2\\) would be \\(1.0\\). But as we now, that will be not the case. So, assume a path of \\(.7\\) for \\(F \\rightarrow X1\\) and \\(.8\\) for \\(F \\rightarrow X2\\). Consequently, there would be a correlation of \\(.56\\) between the two variables \\(X1\\) and \\(X2\\). The measurement error will take into account the difference between \\(1.0\\) and \\(.56\\). Because of this, the relationships between the latent constructs can be more accurate.  Simultaneous Estimation Regarding most methods, only the relationship of a dependent variable to one or more independent variables can be estimated. However, using SEM, it is also possible to estimate the relationship between multiple dependent variables. But the simultaneous estimation gives another advantage. Thus, in addition to the direct effect, the indirect and total effects can be easily calculated. Just a small example: \\(\\small Ind. \\ Effect_{X1\\rightarrow Y} = Dir. \\ Effect_{X1\\rightarrow X2}*Dir. \\ Effect_{X2\\rightarrow Y} = .9*-.47 = -.43\\) \\(\\small Tot. \\ Effect_{X1\\rightarrow Y} = Ind. \\ Effect_{X1\\rightarrow Y} + Dir. \\ Effect_{X1\\rightarrow Y} = -.43 -.42 = -.85\\)\n  Identification Generally, a structural equation model must be identified, otherwise a clear parameter estimation can not be made. The identification should be determined for all model levels:  Identification of the individual measurement models Identification of the structural model Identification of the overall model  The aim should be that there is over-identification at all model levels. In order to determine the degree of identification of a model, the following formula can be used (counting rule): \\[t \\leq\\frac{p(p+1)}{2}\\\\ p = observed \\ variables \\\\ t = estimated\\ paramters\\]\nThe same formula can be used to calculate the degree of freedoms as well:\n\\[df =\\frac{p(p+1)}{2} - t\\]\nConsequently, there are three scenarios for identification:\n \\(df \u0026lt; 0\\): The model is underidentified \\(df = 0\\): The model is exactly identified \\(df \u0026gt; 0\\): The model is overidentified  As mentioned above, the goal should be an overidentified model. This is needed to perform further tests (for example: testing the fit of the model).  Fit Indices At the beginning of SEM, testing the following null hypothesis was considered a good way to assess the model specification: \\[\\sum = \\sum(0)\\]\nWhere \\(\\sum\\) stands for the population covariance matrix and \\(\\sum{0}\\) for the covariance matrix implied by the specified model. To check this null hypothesis, a \\(\\chi^2\\)-test can be used. However, this test is extremely sensitive to high sample sizes1. For this reason, other methods have been developed to test the qualtiy of a model. In the following, some important Fit Indices will be discussed. \\(\\chi^2\\)-Test Despite the high sensitivity with high sample sizes, I would nevertheless explain the test briefly. The \\(\\chi^2\\) is calculated as follows, where O represents the observations and E represents the expected values: \\[\\chi^{2} = \\sum_{i=1}^{n}\\frac{(O_{i}-E_{i})^2}{E_{i}}\\]\nSince the null hypothesis should not be rejected, the test should not be significant. If it comes to a significant result, then the model would have to be rejected or optimized. But as already pointed out, with larger sample size, there will always be a significant result. Based on this test, most models would have to be rejected. For that reason you should not rely on this test.  CFI - Comparative Fit Index In contrast to the \\(\\chi^2\\)-test, the CFI takes into account the sample size and is reliable even for small samples: \\[CFI = 1-\\bigg(\\frac{\\lambda_{k}}{\\lambda_{i}}\\bigg) = 1 - \\bigg(\\frac{max[(\\chi^2_{t}-df_{t}), 0]}{max[(\\chi^2_{t}-df_{t}),(\\chi^2_{n}-df_{n}),0]}\\bigg)\\]\n\\(\\chi^2_{t}\\) and \\(\\chi^2_{n}\\) are the \\(\\chi^2\\) statistics for the target and the baseline model. The degree of freedoms of the target and the null model are represented by \\(df_{t}\\) and \\(df_{n}\\). However, the CFI evaluates the extent to which the tested model is superior to a alternative model in reproducing the observed covariance matrix. The range of the CFI is between \\(0\\) and \\(1\\), whereas a value of \\(1\\) indicates a perfect fit. The cut off value is \\(.95\\)2.  RMSEA - Root Mean Square Error of Approximation RMSEA expresses the discrepancy between the observed covariance matrix and the covariance matrix implied by the model per degree of freedom: \\[RMSEA = \\sqrt\\frac{\\hat{F_{0}}}{df_{t}}=\\sqrt\\frac{max\\Big[\\frac{(\\chi^2_{t}-df_{t})}{(N-1)},0\\Big]}{df_{t}}= \\sqrt\\frac{\\chi^2_{t} - df_{t}}{df_{t}(N-1)}\\]\nWhere \\(\\chi^2_{t}\\) is the \\(\\chi^2\\) statistic and \\(df_{t}\\) the degree of freedom for the target model. In contrast to the CFI, the RMSEA does not perform well with small sample sizes. So, it tends to reject a true model, if the sample size is small. However, the RMSEA does include the model complexity. As with the CFI, there is a range from \\(0\\) to \\(1\\), where a value of \\(0\\) implies a perfect fit. A value of \\(\\leq.05\\) implies a very good fit, a value between \\(.05\\) and \\(.08\\) is considered with a good fit and a value between \\(.08\\) and \\(.10\\) represents a acceptable fit. However, if the RMSEA is greater than \\(.10\\) the fit of the model is bad2.  SRMR - Standardized Root Mean Square Residual The SRMR is the root of the difference between the residuals of the covaruance matrix implied by the specified model and the3: \\[SRMR = \\sqrt{\\frac{\\Bigg\\{2\\sum_{i=1}^{p}\\sum_{j=1}^{i}\\Bigg[\\frac{(s_{ij}-\\hat\\sigma_{ij})}{(s_{ii}*s_{jj})}\\Bigg]^2\\Bigg\\}}{p(p+1)}}\\]\n\\(s_{ii}\\) and \\(s_{jj}\\) are the observed standard deviations. \\(\\sigma_{ij}\\) is the covariance matrix implied by the model, wheras \\(s_{ij}\\) is the observed covariance matrix. A value of \\(0\\) implies a perfect fit. The SRMR should not be greater than \\(.08\\)2.   Model Optimization The modification indices can be used to identify potential model misspecification. If the standardized estimated parameter change (SEPC) is \\(\\geq.20\\), there may be a possible misspecification4. If you want to optimize the model, the parameters can be freely estimated, in which the SEPC is \\(\\geq.20\\). However, this quickly results in parameters being freely estimated where theoretical justification is only partially possible. Thus, error correlations are often freely estimated, which are theoretically rarely well justified5. The fit of the model can indeed be easily optimized in this way. But what brings a better fit, if the resulting model is no longer similar to the theory to be tested? Right, such a model offers little added value. Therefore, a model should only be optimized if this is theoretically well justified.  References 1 Cangur, S. and Ercan, I. 2015: Comparison of model fit indices used in structural equation modeling under multivariate normality. Journal of Modern Applied Statistical Methods. 14 (1), 14.  2 Hu, L.-t. and Bentler, P.M. 1999: Cutoff criteria for fit indexes in covariance structure analysis: Conventional criteria versus new alternatives. Structural Equation Modeling: A Multidisciplinary Journal. 6 (1), 1–55.  3 Chen, F.F. 2007: Sensitivity of goodness of fit indexes to lack of measurement invariance. Structural Equation Modeling. 14 (3), 464–504.  4 Whittaker, T.A. 2012: Using the modification index and standardized expected parameter change for model modification. The Journal of Experimental Education. 80 (1), 26–44.  5 Hermida, R. 2015: The problem of allowing correlated errors in structural equation modeling: Concerns and considerations. Computational Methods in Social Sciences. 3 (1), 5.    ","date":1539388800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539388800,"objectID":"a046aa2becb3418b92874f13d1748b56","permalink":"/post/structural-equation-modeling-a-short-introduction/","publishdate":"2018-10-13T00:00:00Z","relpermalink":"/post/structural-equation-modeling-a-short-introduction/","section":"post","summary":"Introduction \u0026amp; Advantages There are a lot of variations regarding Structural Equation Modeling (SEM). Therefore, this article is focusing on the basics of SEM. Fundamentally, SEM can be classified as a combination of path analysis and confirmatory factor analysis. With SEM and path analysis you will have a big felxibility for specifying relationships between theoretical constructs. For example, it is possible to postulate rather complex models that may include a single construct that is theorized to be a predictor of some constructs and is also predicted by other constructs.","tags":["Structural Equation Modeling","SEM","fit indices","lavaan","semplot"],"title":"Structural Equation Modeling: A Short Introduction","type":"post"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;\n","date":1536444000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536444000,"objectID":"6a451186c775f5f0adb3a0416d0cb711","permalink":"/tutorial/example/","publishdate":"2018-09-09T00:00:00+02:00","relpermalink":"/tutorial/example/","section":"tutorial","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;","tags":null,"title":"Example Page","type":"docs"}]